{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5120910384068279,
  "eval_steps": 500,
  "global_step": 900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002844950213371266,
      "grad_norm": 0.4949305057525635,
      "learning_rate": 0.00019943084803642573,
      "loss": 2.2738,
      "step": 5
    },
    {
      "epoch": 0.005689900426742532,
      "grad_norm": 0.5478770136833191,
      "learning_rate": 0.00019886169607285145,
      "loss": 2.1523,
      "step": 10
    },
    {
      "epoch": 0.008534850640113799,
      "grad_norm": 0.4918110966682434,
      "learning_rate": 0.00019829254410927717,
      "loss": 2.0182,
      "step": 15
    },
    {
      "epoch": 0.011379800853485065,
      "grad_norm": 0.46867555379867554,
      "learning_rate": 0.0001977233921457029,
      "loss": 2.0836,
      "step": 20
    },
    {
      "epoch": 0.01422475106685633,
      "grad_norm": 0.4838064908981323,
      "learning_rate": 0.00019715424018212864,
      "loss": 1.8437,
      "step": 25
    },
    {
      "epoch": 0.017069701280227598,
      "grad_norm": 0.5012277364730835,
      "learning_rate": 0.00019658508821855436,
      "loss": 1.8759,
      "step": 30
    },
    {
      "epoch": 0.01991465149359886,
      "grad_norm": 0.44116437435150146,
      "learning_rate": 0.0001960159362549801,
      "loss": 1.9221,
      "step": 35
    },
    {
      "epoch": 0.02275960170697013,
      "grad_norm": 0.46326640248298645,
      "learning_rate": 0.00019544678429140583,
      "loss": 1.8934,
      "step": 40
    },
    {
      "epoch": 0.025604551920341393,
      "grad_norm": 0.4289257824420929,
      "learning_rate": 0.00019487763232783155,
      "loss": 1.8505,
      "step": 45
    },
    {
      "epoch": 0.02844950213371266,
      "grad_norm": 0.46900564432144165,
      "learning_rate": 0.00019430848036425727,
      "loss": 1.9045,
      "step": 50
    },
    {
      "epoch": 0.031294452347083924,
      "grad_norm": 0.5247617363929749,
      "learning_rate": 0.00019373932840068299,
      "loss": 1.8131,
      "step": 55
    },
    {
      "epoch": 0.034139402560455195,
      "grad_norm": 0.4680875837802887,
      "learning_rate": 0.0001931701764371087,
      "loss": 1.8481,
      "step": 60
    },
    {
      "epoch": 0.03698435277382646,
      "grad_norm": 0.523281455039978,
      "learning_rate": 0.00019260102447353445,
      "loss": 1.9826,
      "step": 65
    },
    {
      "epoch": 0.03982930298719772,
      "grad_norm": 0.4950197637081146,
      "learning_rate": 0.00019203187250996017,
      "loss": 1.8674,
      "step": 70
    },
    {
      "epoch": 0.04267425320056899,
      "grad_norm": 0.5106218457221985,
      "learning_rate": 0.0001914627205463859,
      "loss": 1.8541,
      "step": 75
    },
    {
      "epoch": 0.04551920341394026,
      "grad_norm": 0.46364477276802063,
      "learning_rate": 0.00019089356858281161,
      "loss": 1.8454,
      "step": 80
    },
    {
      "epoch": 0.04836415362731152,
      "grad_norm": 0.5279164910316467,
      "learning_rate": 0.00019032441661923733,
      "loss": 1.7972,
      "step": 85
    },
    {
      "epoch": 0.051209103840682786,
      "grad_norm": 0.48782220482826233,
      "learning_rate": 0.00018975526465566305,
      "loss": 1.8047,
      "step": 90
    },
    {
      "epoch": 0.05405405405405406,
      "grad_norm": 0.5064043402671814,
      "learning_rate": 0.0001891861126920888,
      "loss": 1.7437,
      "step": 95
    },
    {
      "epoch": 0.05689900426742532,
      "grad_norm": 0.5325073003768921,
      "learning_rate": 0.00018861696072851452,
      "loss": 1.8588,
      "step": 100
    },
    {
      "epoch": 0.059743954480796585,
      "grad_norm": 0.4949420392513275,
      "learning_rate": 0.00018804780876494027,
      "loss": 1.9295,
      "step": 105
    },
    {
      "epoch": 0.06258890469416785,
      "grad_norm": 0.5514007806777954,
      "learning_rate": 0.000187478656801366,
      "loss": 1.8105,
      "step": 110
    },
    {
      "epoch": 0.06543385490753911,
      "grad_norm": 0.561600387096405,
      "learning_rate": 0.0001869095048377917,
      "loss": 1.8116,
      "step": 115
    },
    {
      "epoch": 0.06827880512091039,
      "grad_norm": 0.5257200598716736,
      "learning_rate": 0.00018634035287421743,
      "loss": 1.852,
      "step": 120
    },
    {
      "epoch": 0.07112375533428165,
      "grad_norm": 0.4855252802371979,
      "learning_rate": 0.00018577120091064315,
      "loss": 1.8639,
      "step": 125
    },
    {
      "epoch": 0.07396870554765292,
      "grad_norm": 0.5099745988845825,
      "learning_rate": 0.00018520204894706887,
      "loss": 1.7894,
      "step": 130
    },
    {
      "epoch": 0.07681365576102418,
      "grad_norm": 0.4951542317867279,
      "learning_rate": 0.0001846328969834946,
      "loss": 1.7587,
      "step": 135
    },
    {
      "epoch": 0.07965860597439545,
      "grad_norm": 0.5255674719810486,
      "learning_rate": 0.0001840637450199203,
      "loss": 1.9911,
      "step": 140
    },
    {
      "epoch": 0.08250355618776671,
      "grad_norm": 0.4962505102157593,
      "learning_rate": 0.00018349459305634603,
      "loss": 1.7364,
      "step": 145
    },
    {
      "epoch": 0.08534850640113797,
      "grad_norm": 0.49747973680496216,
      "learning_rate": 0.00018292544109277178,
      "loss": 1.8365,
      "step": 150
    },
    {
      "epoch": 0.08819345661450925,
      "grad_norm": 0.5000907182693481,
      "learning_rate": 0.0001823562891291975,
      "loss": 1.7884,
      "step": 155
    },
    {
      "epoch": 0.09103840682788052,
      "grad_norm": 0.5320129990577698,
      "learning_rate": 0.00018178713716562325,
      "loss": 1.8548,
      "step": 160
    },
    {
      "epoch": 0.09388335704125178,
      "grad_norm": 0.5494191646575928,
      "learning_rate": 0.00018121798520204897,
      "loss": 1.8061,
      "step": 165
    },
    {
      "epoch": 0.09672830725462304,
      "grad_norm": 0.5762655138969421,
      "learning_rate": 0.0001806488332384747,
      "loss": 1.8744,
      "step": 170
    },
    {
      "epoch": 0.09957325746799431,
      "grad_norm": 0.5443115830421448,
      "learning_rate": 0.0001800796812749004,
      "loss": 1.9422,
      "step": 175
    },
    {
      "epoch": 0.10241820768136557,
      "grad_norm": 0.5426173210144043,
      "learning_rate": 0.00017951052931132613,
      "loss": 1.9794,
      "step": 180
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 0.5198436379432678,
      "learning_rate": 0.00017894137734775185,
      "loss": 1.8975,
      "step": 185
    },
    {
      "epoch": 0.10810810810810811,
      "grad_norm": 0.5192174911499023,
      "learning_rate": 0.0001783722253841776,
      "loss": 1.9407,
      "step": 190
    },
    {
      "epoch": 0.11095305832147938,
      "grad_norm": 0.5123754739761353,
      "learning_rate": 0.00017780307342060332,
      "loss": 1.7721,
      "step": 195
    },
    {
      "epoch": 0.11379800853485064,
      "grad_norm": 0.5444467067718506,
      "learning_rate": 0.00017723392145702904,
      "loss": 1.7668,
      "step": 200
    },
    {
      "epoch": 0.1166429587482219,
      "grad_norm": 0.48558250069618225,
      "learning_rate": 0.00017666476949345476,
      "loss": 1.8502,
      "step": 205
    },
    {
      "epoch": 0.11948790896159317,
      "grad_norm": 0.5235543847084045,
      "learning_rate": 0.00017609561752988048,
      "loss": 1.7303,
      "step": 210
    },
    {
      "epoch": 0.12233285917496443,
      "grad_norm": 0.5079710483551025,
      "learning_rate": 0.0001755264655663062,
      "loss": 1.6375,
      "step": 215
    },
    {
      "epoch": 0.1251778093883357,
      "grad_norm": 0.5100827813148499,
      "learning_rate": 0.00017495731360273194,
      "loss": 1.7996,
      "step": 220
    },
    {
      "epoch": 0.12802275960170698,
      "grad_norm": 0.524852991104126,
      "learning_rate": 0.00017438816163915766,
      "loss": 1.7877,
      "step": 225
    },
    {
      "epoch": 0.13086770981507823,
      "grad_norm": 0.5219963788986206,
      "learning_rate": 0.0001738190096755834,
      "loss": 1.8222,
      "step": 230
    },
    {
      "epoch": 0.1337126600284495,
      "grad_norm": 0.54615318775177,
      "learning_rate": 0.00017324985771200913,
      "loss": 1.8048,
      "step": 235
    },
    {
      "epoch": 0.13655761024182078,
      "grad_norm": 0.4962216913700104,
      "learning_rate": 0.00017268070574843485,
      "loss": 1.7844,
      "step": 240
    },
    {
      "epoch": 0.13940256045519203,
      "grad_norm": 0.5254645943641663,
      "learning_rate": 0.00017211155378486057,
      "loss": 1.806,
      "step": 245
    },
    {
      "epoch": 0.1422475106685633,
      "grad_norm": 0.5344192981719971,
      "learning_rate": 0.0001715424018212863,
      "loss": 1.7977,
      "step": 250
    },
    {
      "epoch": 0.14509246088193456,
      "grad_norm": 0.5260816216468811,
      "learning_rate": 0.000170973249857712,
      "loss": 1.7991,
      "step": 255
    },
    {
      "epoch": 0.14793741109530584,
      "grad_norm": 0.536462664604187,
      "learning_rate": 0.00017040409789413773,
      "loss": 1.8164,
      "step": 260
    },
    {
      "epoch": 0.1507823613086771,
      "grad_norm": 0.534024715423584,
      "learning_rate": 0.00016983494593056345,
      "loss": 1.8092,
      "step": 265
    },
    {
      "epoch": 0.15362731152204837,
      "grad_norm": 0.5141861438751221,
      "learning_rate": 0.00016926579396698917,
      "loss": 1.8453,
      "step": 270
    },
    {
      "epoch": 0.15647226173541964,
      "grad_norm": 0.5123191475868225,
      "learning_rate": 0.00016869664200341492,
      "loss": 1.8227,
      "step": 275
    },
    {
      "epoch": 0.1593172119487909,
      "grad_norm": 0.5313526391983032,
      "learning_rate": 0.00016812749003984064,
      "loss": 1.7419,
      "step": 280
    },
    {
      "epoch": 0.16216216216216217,
      "grad_norm": 0.5069981813430786,
      "learning_rate": 0.0001675583380762664,
      "loss": 1.8631,
      "step": 285
    },
    {
      "epoch": 0.16500711237553342,
      "grad_norm": 0.5520071387290955,
      "learning_rate": 0.0001669891861126921,
      "loss": 1.8749,
      "step": 290
    },
    {
      "epoch": 0.1678520625889047,
      "grad_norm": 0.5548297166824341,
      "learning_rate": 0.00016642003414911783,
      "loss": 1.8134,
      "step": 295
    },
    {
      "epoch": 0.17069701280227595,
      "grad_norm": 0.5602995157241821,
      "learning_rate": 0.00016585088218554355,
      "loss": 1.8562,
      "step": 300
    },
    {
      "epoch": 0.17354196301564723,
      "grad_norm": 0.584997832775116,
      "learning_rate": 0.00016528173022196927,
      "loss": 1.6884,
      "step": 305
    },
    {
      "epoch": 0.1763869132290185,
      "grad_norm": 0.5142406821250916,
      "learning_rate": 0.000164712578258395,
      "loss": 1.7798,
      "step": 310
    },
    {
      "epoch": 0.17923186344238975,
      "grad_norm": 0.542140543460846,
      "learning_rate": 0.00016414342629482074,
      "loss": 1.8115,
      "step": 315
    },
    {
      "epoch": 0.18207681365576103,
      "grad_norm": 0.5546067953109741,
      "learning_rate": 0.00016357427433124646,
      "loss": 1.8028,
      "step": 320
    },
    {
      "epoch": 0.18492176386913228,
      "grad_norm": 0.5099424719810486,
      "learning_rate": 0.00016300512236767218,
      "loss": 1.7713,
      "step": 325
    },
    {
      "epoch": 0.18776671408250356,
      "grad_norm": 0.5469363927841187,
      "learning_rate": 0.0001624359704040979,
      "loss": 1.7708,
      "step": 330
    },
    {
      "epoch": 0.1906116642958748,
      "grad_norm": 0.5607678890228271,
      "learning_rate": 0.00016186681844052362,
      "loss": 1.854,
      "step": 335
    },
    {
      "epoch": 0.1934566145092461,
      "grad_norm": 0.550093412399292,
      "learning_rate": 0.00016129766647694934,
      "loss": 1.7278,
      "step": 340
    },
    {
      "epoch": 0.19630156472261737,
      "grad_norm": 0.6458361744880676,
      "learning_rate": 0.00016072851451337508,
      "loss": 1.8848,
      "step": 345
    },
    {
      "epoch": 0.19914651493598862,
      "grad_norm": 0.5263909101486206,
      "learning_rate": 0.0001601593625498008,
      "loss": 1.7014,
      "step": 350
    },
    {
      "epoch": 0.2019914651493599,
      "grad_norm": 0.5069615244865417,
      "learning_rate": 0.00015959021058622655,
      "loss": 1.7338,
      "step": 355
    },
    {
      "epoch": 0.20483641536273114,
      "grad_norm": 0.5174620151519775,
      "learning_rate": 0.00015902105862265227,
      "loss": 1.711,
      "step": 360
    },
    {
      "epoch": 0.20768136557610242,
      "grad_norm": 0.536195695400238,
      "learning_rate": 0.000158451906659078,
      "loss": 1.8283,
      "step": 365
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 0.5349156856536865,
      "learning_rate": 0.0001578827546955037,
      "loss": 1.8303,
      "step": 370
    },
    {
      "epoch": 0.21337126600284495,
      "grad_norm": 0.5570517778396606,
      "learning_rate": 0.00015731360273192943,
      "loss": 1.6737,
      "step": 375
    },
    {
      "epoch": 0.21621621621621623,
      "grad_norm": 0.5562851428985596,
      "learning_rate": 0.00015674445076835515,
      "loss": 1.7031,
      "step": 380
    },
    {
      "epoch": 0.21906116642958748,
      "grad_norm": 0.5085716843605042,
      "learning_rate": 0.00015617529880478087,
      "loss": 1.7839,
      "step": 385
    },
    {
      "epoch": 0.22190611664295876,
      "grad_norm": 0.5509480834007263,
      "learning_rate": 0.0001556061468412066,
      "loss": 1.8322,
      "step": 390
    },
    {
      "epoch": 0.22475106685633,
      "grad_norm": 0.552862286567688,
      "learning_rate": 0.00015503699487763231,
      "loss": 1.8205,
      "step": 395
    },
    {
      "epoch": 0.22759601706970128,
      "grad_norm": 0.5689870119094849,
      "learning_rate": 0.00015446784291405806,
      "loss": 1.8252,
      "step": 400
    },
    {
      "epoch": 0.23044096728307253,
      "grad_norm": 0.5379787683486938,
      "learning_rate": 0.00015389869095048378,
      "loss": 1.7698,
      "step": 405
    },
    {
      "epoch": 0.2332859174964438,
      "grad_norm": 0.5686361193656921,
      "learning_rate": 0.00015332953898690953,
      "loss": 1.81,
      "step": 410
    },
    {
      "epoch": 0.2361308677098151,
      "grad_norm": 0.5446853041648865,
      "learning_rate": 0.00015276038702333525,
      "loss": 1.741,
      "step": 415
    },
    {
      "epoch": 0.23897581792318634,
      "grad_norm": 0.549289882183075,
      "learning_rate": 0.00015219123505976097,
      "loss": 1.7816,
      "step": 420
    },
    {
      "epoch": 0.24182076813655762,
      "grad_norm": 0.5685895085334778,
      "learning_rate": 0.0001516220830961867,
      "loss": 1.7425,
      "step": 425
    },
    {
      "epoch": 0.24466571834992887,
      "grad_norm": 0.5328943729400635,
      "learning_rate": 0.0001510529311326124,
      "loss": 1.7754,
      "step": 430
    },
    {
      "epoch": 0.24751066856330015,
      "grad_norm": 0.5207207202911377,
      "learning_rate": 0.00015048377916903813,
      "loss": 1.7066,
      "step": 435
    },
    {
      "epoch": 0.2503556187766714,
      "grad_norm": 0.5533854961395264,
      "learning_rate": 0.00014991462720546388,
      "loss": 1.8044,
      "step": 440
    },
    {
      "epoch": 0.2532005689900427,
      "grad_norm": 0.5449127554893494,
      "learning_rate": 0.0001493454752418896,
      "loss": 1.7322,
      "step": 445
    },
    {
      "epoch": 0.25604551920341395,
      "grad_norm": 0.5602892637252808,
      "learning_rate": 0.00014877632327831532,
      "loss": 1.8496,
      "step": 450
    },
    {
      "epoch": 0.25889046941678523,
      "grad_norm": 0.6014001965522766,
      "learning_rate": 0.00014820717131474104,
      "loss": 1.7352,
      "step": 455
    },
    {
      "epoch": 0.26173541963015645,
      "grad_norm": 0.5059030652046204,
      "learning_rate": 0.00014763801935116676,
      "loss": 1.7903,
      "step": 460
    },
    {
      "epoch": 0.26458036984352773,
      "grad_norm": 0.5593153834342957,
      "learning_rate": 0.00014706886738759248,
      "loss": 1.8549,
      "step": 465
    },
    {
      "epoch": 0.267425320056899,
      "grad_norm": 0.5455724000930786,
      "learning_rate": 0.00014649971542401823,
      "loss": 1.8233,
      "step": 470
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 0.5374266505241394,
      "learning_rate": 0.00014593056346044395,
      "loss": 1.7071,
      "step": 475
    },
    {
      "epoch": 0.27311522048364156,
      "grad_norm": 0.5489568114280701,
      "learning_rate": 0.0001453614114968697,
      "loss": 1.7843,
      "step": 480
    },
    {
      "epoch": 0.2759601706970128,
      "grad_norm": 0.5015942454338074,
      "learning_rate": 0.00014479225953329541,
      "loss": 1.8701,
      "step": 485
    },
    {
      "epoch": 0.27880512091038406,
      "grad_norm": 0.5308811068534851,
      "learning_rate": 0.00014422310756972113,
      "loss": 1.8204,
      "step": 490
    },
    {
      "epoch": 0.28165007112375534,
      "grad_norm": 0.5483800768852234,
      "learning_rate": 0.00014365395560614685,
      "loss": 1.7381,
      "step": 495
    },
    {
      "epoch": 0.2844950213371266,
      "grad_norm": 0.541336715221405,
      "learning_rate": 0.00014308480364257257,
      "loss": 1.7185,
      "step": 500
    },
    {
      "epoch": 0.28733997155049784,
      "grad_norm": 0.56029212474823,
      "learning_rate": 0.0001425156516789983,
      "loss": 1.7201,
      "step": 505
    },
    {
      "epoch": 0.2901849217638691,
      "grad_norm": 0.5436985492706299,
      "learning_rate": 0.00014194649971542401,
      "loss": 1.8067,
      "step": 510
    },
    {
      "epoch": 0.2930298719772404,
      "grad_norm": 0.5450965762138367,
      "learning_rate": 0.00014137734775184974,
      "loss": 1.8599,
      "step": 515
    },
    {
      "epoch": 0.2958748221906117,
      "grad_norm": 0.5704220533370972,
      "learning_rate": 0.00014080819578827546,
      "loss": 1.7425,
      "step": 520
    },
    {
      "epoch": 0.29871977240398295,
      "grad_norm": 0.5314556360244751,
      "learning_rate": 0.0001402390438247012,
      "loss": 1.779,
      "step": 525
    },
    {
      "epoch": 0.3015647226173542,
      "grad_norm": 0.5982683897018433,
      "learning_rate": 0.00013966989186112692,
      "loss": 1.8211,
      "step": 530
    },
    {
      "epoch": 0.30440967283072545,
      "grad_norm": 0.5527154207229614,
      "learning_rate": 0.00013910073989755264,
      "loss": 1.7719,
      "step": 535
    },
    {
      "epoch": 0.30725462304409673,
      "grad_norm": 0.5959969162940979,
      "learning_rate": 0.0001385315879339784,
      "loss": 1.8675,
      "step": 540
    },
    {
      "epoch": 0.310099573257468,
      "grad_norm": 0.5495197176933289,
      "learning_rate": 0.0001379624359704041,
      "loss": 1.7607,
      "step": 545
    },
    {
      "epoch": 0.3129445234708393,
      "grad_norm": 0.5293635129928589,
      "learning_rate": 0.00013739328400682983,
      "loss": 1.664,
      "step": 550
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 0.5742693543434143,
      "learning_rate": 0.00013682413204325555,
      "loss": 1.7203,
      "step": 555
    },
    {
      "epoch": 0.3186344238975818,
      "grad_norm": 0.5422243475914001,
      "learning_rate": 0.00013625498007968127,
      "loss": 1.7767,
      "step": 560
    },
    {
      "epoch": 0.32147937411095306,
      "grad_norm": 0.5649269223213196,
      "learning_rate": 0.00013568582811610702,
      "loss": 1.8364,
      "step": 565
    },
    {
      "epoch": 0.32432432432432434,
      "grad_norm": 0.5209592580795288,
      "learning_rate": 0.00013511667615253274,
      "loss": 1.7459,
      "step": 570
    },
    {
      "epoch": 0.32716927453769556,
      "grad_norm": 0.5393198132514954,
      "learning_rate": 0.00013454752418895846,
      "loss": 1.7314,
      "step": 575
    },
    {
      "epoch": 0.33001422475106684,
      "grad_norm": 0.5036423802375793,
      "learning_rate": 0.00013397837222538418,
      "loss": 1.7298,
      "step": 580
    },
    {
      "epoch": 0.3328591749644381,
      "grad_norm": 0.5882377028465271,
      "learning_rate": 0.0001334092202618099,
      "loss": 1.7044,
      "step": 585
    },
    {
      "epoch": 0.3357041251778094,
      "grad_norm": 0.5572733879089355,
      "learning_rate": 0.00013284006829823562,
      "loss": 1.7219,
      "step": 590
    },
    {
      "epoch": 0.3385490753911807,
      "grad_norm": 0.5847784876823425,
      "learning_rate": 0.00013227091633466137,
      "loss": 1.8744,
      "step": 595
    },
    {
      "epoch": 0.3413940256045519,
      "grad_norm": 0.5702312588691711,
      "learning_rate": 0.0001317017643710871,
      "loss": 1.7204,
      "step": 600
    },
    {
      "epoch": 0.3442389758179232,
      "grad_norm": 0.5564872026443481,
      "learning_rate": 0.00013113261240751283,
      "loss": 1.7727,
      "step": 605
    },
    {
      "epoch": 0.34708392603129445,
      "grad_norm": 0.540156900882721,
      "learning_rate": 0.00013056346044393855,
      "loss": 1.7603,
      "step": 610
    },
    {
      "epoch": 0.34992887624466573,
      "grad_norm": 0.5652338862419128,
      "learning_rate": 0.00012999430848036428,
      "loss": 1.7419,
      "step": 615
    },
    {
      "epoch": 0.352773826458037,
      "grad_norm": 0.5541778206825256,
      "learning_rate": 0.00012942515651679,
      "loss": 1.7433,
      "step": 620
    },
    {
      "epoch": 0.35561877667140823,
      "grad_norm": 0.6089360117912292,
      "learning_rate": 0.00012885600455321572,
      "loss": 1.7459,
      "step": 625
    },
    {
      "epoch": 0.3584637268847795,
      "grad_norm": 0.5532107353210449,
      "learning_rate": 0.00012828685258964144,
      "loss": 1.7636,
      "step": 630
    },
    {
      "epoch": 0.3613086770981508,
      "grad_norm": 0.5951501131057739,
      "learning_rate": 0.00012771770062606716,
      "loss": 1.7711,
      "step": 635
    },
    {
      "epoch": 0.36415362731152207,
      "grad_norm": 0.530169665813446,
      "learning_rate": 0.00012714854866249288,
      "loss": 1.7152,
      "step": 640
    },
    {
      "epoch": 0.3669985775248933,
      "grad_norm": 0.539699137210846,
      "learning_rate": 0.0001265793966989186,
      "loss": 1.7072,
      "step": 645
    },
    {
      "epoch": 0.36984352773826457,
      "grad_norm": 0.5301237106323242,
      "learning_rate": 0.00012601024473534434,
      "loss": 1.7513,
      "step": 650
    },
    {
      "epoch": 0.37268847795163584,
      "grad_norm": 0.5763750076293945,
      "learning_rate": 0.00012544109277177006,
      "loss": 1.793,
      "step": 655
    },
    {
      "epoch": 0.3755334281650071,
      "grad_norm": 0.5494654774665833,
      "learning_rate": 0.00012487194080819578,
      "loss": 1.6776,
      "step": 660
    },
    {
      "epoch": 0.3783783783783784,
      "grad_norm": 0.5408105850219727,
      "learning_rate": 0.00012430278884462153,
      "loss": 1.7445,
      "step": 665
    },
    {
      "epoch": 0.3812233285917496,
      "grad_norm": 0.6453489065170288,
      "learning_rate": 0.00012373363688104725,
      "loss": 1.7409,
      "step": 670
    },
    {
      "epoch": 0.3840682788051209,
      "grad_norm": 0.5595253109931946,
      "learning_rate": 0.00012316448491747297,
      "loss": 1.6965,
      "step": 675
    },
    {
      "epoch": 0.3869132290184922,
      "grad_norm": 0.564319372177124,
      "learning_rate": 0.0001225953329538987,
      "loss": 1.7338,
      "step": 680
    },
    {
      "epoch": 0.38975817923186346,
      "grad_norm": 0.5440504550933838,
      "learning_rate": 0.00012202618099032441,
      "loss": 1.9428,
      "step": 685
    },
    {
      "epoch": 0.39260312944523473,
      "grad_norm": 0.6019558310508728,
      "learning_rate": 0.00012145702902675016,
      "loss": 1.8047,
      "step": 690
    },
    {
      "epoch": 0.39544807965860596,
      "grad_norm": 0.5315166711807251,
      "learning_rate": 0.00012088787706317588,
      "loss": 1.7543,
      "step": 695
    },
    {
      "epoch": 0.39829302987197723,
      "grad_norm": 0.5731531977653503,
      "learning_rate": 0.0001203187250996016,
      "loss": 1.7612,
      "step": 700
    },
    {
      "epoch": 0.4011379800853485,
      "grad_norm": 0.5607674717903137,
      "learning_rate": 0.00011974957313602732,
      "loss": 1.5737,
      "step": 705
    },
    {
      "epoch": 0.4039829302987198,
      "grad_norm": 0.5875958800315857,
      "learning_rate": 0.00011918042117245305,
      "loss": 1.7778,
      "step": 710
    },
    {
      "epoch": 0.406827880512091,
      "grad_norm": 0.5785971283912659,
      "learning_rate": 0.00011861126920887877,
      "loss": 1.8312,
      "step": 715
    },
    {
      "epoch": 0.4096728307254623,
      "grad_norm": 0.5557567477226257,
      "learning_rate": 0.0001180421172453045,
      "loss": 1.7575,
      "step": 720
    },
    {
      "epoch": 0.41251778093883357,
      "grad_norm": 0.5731320381164551,
      "learning_rate": 0.00011747296528173021,
      "loss": 1.6867,
      "step": 725
    },
    {
      "epoch": 0.41536273115220484,
      "grad_norm": 0.5766249299049377,
      "learning_rate": 0.00011690381331815596,
      "loss": 1.854,
      "step": 730
    },
    {
      "epoch": 0.4182076813655761,
      "grad_norm": 0.5616649985313416,
      "learning_rate": 0.00011633466135458168,
      "loss": 1.7036,
      "step": 735
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 0.5768249034881592,
      "learning_rate": 0.00011576550939100742,
      "loss": 1.7663,
      "step": 740
    },
    {
      "epoch": 0.4238975817923186,
      "grad_norm": 0.5535956025123596,
      "learning_rate": 0.00011519635742743314,
      "loss": 1.6985,
      "step": 745
    },
    {
      "epoch": 0.4267425320056899,
      "grad_norm": 0.6212028861045837,
      "learning_rate": 0.00011462720546385886,
      "loss": 1.7025,
      "step": 750
    },
    {
      "epoch": 0.4295874822190612,
      "grad_norm": 0.5723338723182678,
      "learning_rate": 0.00011405805350028458,
      "loss": 1.6444,
      "step": 755
    },
    {
      "epoch": 0.43243243243243246,
      "grad_norm": 0.611225426197052,
      "learning_rate": 0.0001134889015367103,
      "loss": 1.7765,
      "step": 760
    },
    {
      "epoch": 0.4352773826458037,
      "grad_norm": 0.566222608089447,
      "learning_rate": 0.00011291974957313603,
      "loss": 1.733,
      "step": 765
    },
    {
      "epoch": 0.43812233285917496,
      "grad_norm": 0.6035428643226624,
      "learning_rate": 0.00011235059760956175,
      "loss": 1.7163,
      "step": 770
    },
    {
      "epoch": 0.44096728307254623,
      "grad_norm": 0.5727686285972595,
      "learning_rate": 0.0001117814456459875,
      "loss": 1.7746,
      "step": 775
    },
    {
      "epoch": 0.4438122332859175,
      "grad_norm": 0.6219565272331238,
      "learning_rate": 0.00011121229368241322,
      "loss": 1.8464,
      "step": 780
    },
    {
      "epoch": 0.4466571834992888,
      "grad_norm": 0.5219248533248901,
      "learning_rate": 0.00011064314171883894,
      "loss": 1.7434,
      "step": 785
    },
    {
      "epoch": 0.44950213371266,
      "grad_norm": 0.582021176815033,
      "learning_rate": 0.00011007398975526466,
      "loss": 1.721,
      "step": 790
    },
    {
      "epoch": 0.4523470839260313,
      "grad_norm": 0.5642722845077515,
      "learning_rate": 0.00010950483779169038,
      "loss": 1.7102,
      "step": 795
    },
    {
      "epoch": 0.45519203413940257,
      "grad_norm": 0.5546287894248962,
      "learning_rate": 0.00010893568582811611,
      "loss": 1.7288,
      "step": 800
    },
    {
      "epoch": 0.45803698435277385,
      "grad_norm": 0.5949670076370239,
      "learning_rate": 0.00010836653386454183,
      "loss": 1.6573,
      "step": 805
    },
    {
      "epoch": 0.46088193456614507,
      "grad_norm": 0.5746669173240662,
      "learning_rate": 0.00010779738190096755,
      "loss": 1.6618,
      "step": 810
    },
    {
      "epoch": 0.46372688477951635,
      "grad_norm": 0.5729794502258301,
      "learning_rate": 0.0001072282299373933,
      "loss": 1.8412,
      "step": 815
    },
    {
      "epoch": 0.4665718349928876,
      "grad_norm": 0.5517909526824951,
      "learning_rate": 0.00010665907797381902,
      "loss": 1.8665,
      "step": 820
    },
    {
      "epoch": 0.4694167852062589,
      "grad_norm": 0.5782808661460876,
      "learning_rate": 0.00010608992601024474,
      "loss": 1.7508,
      "step": 825
    },
    {
      "epoch": 0.4722617354196302,
      "grad_norm": 0.5499626994132996,
      "learning_rate": 0.00010552077404667046,
      "loss": 1.6991,
      "step": 830
    },
    {
      "epoch": 0.4751066856330014,
      "grad_norm": 0.61516934633255,
      "learning_rate": 0.0001049516220830962,
      "loss": 1.8193,
      "step": 835
    },
    {
      "epoch": 0.4779516358463727,
      "grad_norm": 0.5890849828720093,
      "learning_rate": 0.00010438247011952192,
      "loss": 1.7798,
      "step": 840
    },
    {
      "epoch": 0.48079658605974396,
      "grad_norm": 0.583966851234436,
      "learning_rate": 0.00010381331815594764,
      "loss": 1.8858,
      "step": 845
    },
    {
      "epoch": 0.48364153627311524,
      "grad_norm": 0.5633518099784851,
      "learning_rate": 0.00010324416619237336,
      "loss": 1.6819,
      "step": 850
    },
    {
      "epoch": 0.4864864864864865,
      "grad_norm": 0.5884907841682434,
      "learning_rate": 0.0001026750142287991,
      "loss": 1.7301,
      "step": 855
    },
    {
      "epoch": 0.48933143669985774,
      "grad_norm": 0.5635905265808105,
      "learning_rate": 0.00010210586226522482,
      "loss": 1.6595,
      "step": 860
    },
    {
      "epoch": 0.492176386913229,
      "grad_norm": 0.6026513576507568,
      "learning_rate": 0.00010153671030165054,
      "loss": 1.6527,
      "step": 865
    },
    {
      "epoch": 0.4950213371266003,
      "grad_norm": 0.5916198492050171,
      "learning_rate": 0.00010096755833807628,
      "loss": 1.6546,
      "step": 870
    },
    {
      "epoch": 0.49786628733997157,
      "grad_norm": 0.5900142192840576,
      "learning_rate": 0.000100398406374502,
      "loss": 1.7762,
      "step": 875
    },
    {
      "epoch": 0.5007112375533428,
      "grad_norm": 0.5492935180664062,
      "learning_rate": 9.982925441092772e-05,
      "loss": 1.7851,
      "step": 880
    },
    {
      "epoch": 0.5035561877667141,
      "grad_norm": 0.5612053871154785,
      "learning_rate": 9.926010244735345e-05,
      "loss": 1.7455,
      "step": 885
    },
    {
      "epoch": 0.5064011379800853,
      "grad_norm": 0.5632994174957275,
      "learning_rate": 9.869095048377917e-05,
      "loss": 1.8396,
      "step": 890
    },
    {
      "epoch": 0.5092460881934566,
      "grad_norm": 0.5755983591079712,
      "learning_rate": 9.81217985202049e-05,
      "loss": 1.6753,
      "step": 895
    },
    {
      "epoch": 0.5120910384068279,
      "grad_norm": 0.7010552883148193,
      "learning_rate": 9.755264655663063e-05,
      "loss": 1.9816,
      "step": 900
    }
  ],
  "logging_steps": 5,
  "max_steps": 1757,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.472919428654592e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
