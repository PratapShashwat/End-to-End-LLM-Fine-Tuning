{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7112375533428165,
  "eval_steps": 500,
  "global_step": 1250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002844950213371266,
      "grad_norm": 0.4949305057525635,
      "learning_rate": 0.00019943084803642573,
      "loss": 2.2738,
      "step": 5
    },
    {
      "epoch": 0.005689900426742532,
      "grad_norm": 0.5478770136833191,
      "learning_rate": 0.00019886169607285145,
      "loss": 2.1523,
      "step": 10
    },
    {
      "epoch": 0.008534850640113799,
      "grad_norm": 0.4918110966682434,
      "learning_rate": 0.00019829254410927717,
      "loss": 2.0182,
      "step": 15
    },
    {
      "epoch": 0.011379800853485065,
      "grad_norm": 0.46867555379867554,
      "learning_rate": 0.0001977233921457029,
      "loss": 2.0836,
      "step": 20
    },
    {
      "epoch": 0.01422475106685633,
      "grad_norm": 0.4838064908981323,
      "learning_rate": 0.00019715424018212864,
      "loss": 1.8437,
      "step": 25
    },
    {
      "epoch": 0.017069701280227598,
      "grad_norm": 0.5012277364730835,
      "learning_rate": 0.00019658508821855436,
      "loss": 1.8759,
      "step": 30
    },
    {
      "epoch": 0.01991465149359886,
      "grad_norm": 0.44116437435150146,
      "learning_rate": 0.0001960159362549801,
      "loss": 1.9221,
      "step": 35
    },
    {
      "epoch": 0.02275960170697013,
      "grad_norm": 0.46326640248298645,
      "learning_rate": 0.00019544678429140583,
      "loss": 1.8934,
      "step": 40
    },
    {
      "epoch": 0.025604551920341393,
      "grad_norm": 0.4289257824420929,
      "learning_rate": 0.00019487763232783155,
      "loss": 1.8505,
      "step": 45
    },
    {
      "epoch": 0.02844950213371266,
      "grad_norm": 0.46900564432144165,
      "learning_rate": 0.00019430848036425727,
      "loss": 1.9045,
      "step": 50
    },
    {
      "epoch": 0.031294452347083924,
      "grad_norm": 0.5247617363929749,
      "learning_rate": 0.00019373932840068299,
      "loss": 1.8131,
      "step": 55
    },
    {
      "epoch": 0.034139402560455195,
      "grad_norm": 0.4680875837802887,
      "learning_rate": 0.0001931701764371087,
      "loss": 1.8481,
      "step": 60
    },
    {
      "epoch": 0.03698435277382646,
      "grad_norm": 0.523281455039978,
      "learning_rate": 0.00019260102447353445,
      "loss": 1.9826,
      "step": 65
    },
    {
      "epoch": 0.03982930298719772,
      "grad_norm": 0.4950197637081146,
      "learning_rate": 0.00019203187250996017,
      "loss": 1.8674,
      "step": 70
    },
    {
      "epoch": 0.04267425320056899,
      "grad_norm": 0.5106218457221985,
      "learning_rate": 0.0001914627205463859,
      "loss": 1.8541,
      "step": 75
    },
    {
      "epoch": 0.04551920341394026,
      "grad_norm": 0.46364477276802063,
      "learning_rate": 0.00019089356858281161,
      "loss": 1.8454,
      "step": 80
    },
    {
      "epoch": 0.04836415362731152,
      "grad_norm": 0.5279164910316467,
      "learning_rate": 0.00019032441661923733,
      "loss": 1.7972,
      "step": 85
    },
    {
      "epoch": 0.051209103840682786,
      "grad_norm": 0.48782220482826233,
      "learning_rate": 0.00018975526465566305,
      "loss": 1.8047,
      "step": 90
    },
    {
      "epoch": 0.05405405405405406,
      "grad_norm": 0.5064043402671814,
      "learning_rate": 0.0001891861126920888,
      "loss": 1.7437,
      "step": 95
    },
    {
      "epoch": 0.05689900426742532,
      "grad_norm": 0.5325073003768921,
      "learning_rate": 0.00018861696072851452,
      "loss": 1.8588,
      "step": 100
    },
    {
      "epoch": 0.059743954480796585,
      "grad_norm": 0.4949420392513275,
      "learning_rate": 0.00018804780876494027,
      "loss": 1.9295,
      "step": 105
    },
    {
      "epoch": 0.06258890469416785,
      "grad_norm": 0.5514007806777954,
      "learning_rate": 0.000187478656801366,
      "loss": 1.8105,
      "step": 110
    },
    {
      "epoch": 0.06543385490753911,
      "grad_norm": 0.561600387096405,
      "learning_rate": 0.0001869095048377917,
      "loss": 1.8116,
      "step": 115
    },
    {
      "epoch": 0.06827880512091039,
      "grad_norm": 0.5257200598716736,
      "learning_rate": 0.00018634035287421743,
      "loss": 1.852,
      "step": 120
    },
    {
      "epoch": 0.07112375533428165,
      "grad_norm": 0.4855252802371979,
      "learning_rate": 0.00018577120091064315,
      "loss": 1.8639,
      "step": 125
    },
    {
      "epoch": 0.07396870554765292,
      "grad_norm": 0.5099745988845825,
      "learning_rate": 0.00018520204894706887,
      "loss": 1.7894,
      "step": 130
    },
    {
      "epoch": 0.07681365576102418,
      "grad_norm": 0.4951542317867279,
      "learning_rate": 0.0001846328969834946,
      "loss": 1.7587,
      "step": 135
    },
    {
      "epoch": 0.07965860597439545,
      "grad_norm": 0.5255674719810486,
      "learning_rate": 0.0001840637450199203,
      "loss": 1.9911,
      "step": 140
    },
    {
      "epoch": 0.08250355618776671,
      "grad_norm": 0.4962505102157593,
      "learning_rate": 0.00018349459305634603,
      "loss": 1.7364,
      "step": 145
    },
    {
      "epoch": 0.08534850640113797,
      "grad_norm": 0.49747973680496216,
      "learning_rate": 0.00018292544109277178,
      "loss": 1.8365,
      "step": 150
    },
    {
      "epoch": 0.08819345661450925,
      "grad_norm": 0.5000907182693481,
      "learning_rate": 0.0001823562891291975,
      "loss": 1.7884,
      "step": 155
    },
    {
      "epoch": 0.09103840682788052,
      "grad_norm": 0.5320129990577698,
      "learning_rate": 0.00018178713716562325,
      "loss": 1.8548,
      "step": 160
    },
    {
      "epoch": 0.09388335704125178,
      "grad_norm": 0.5494191646575928,
      "learning_rate": 0.00018121798520204897,
      "loss": 1.8061,
      "step": 165
    },
    {
      "epoch": 0.09672830725462304,
      "grad_norm": 0.5762655138969421,
      "learning_rate": 0.0001806488332384747,
      "loss": 1.8744,
      "step": 170
    },
    {
      "epoch": 0.09957325746799431,
      "grad_norm": 0.5443115830421448,
      "learning_rate": 0.0001800796812749004,
      "loss": 1.9422,
      "step": 175
    },
    {
      "epoch": 0.10241820768136557,
      "grad_norm": 0.5426173210144043,
      "learning_rate": 0.00017951052931132613,
      "loss": 1.9794,
      "step": 180
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 0.5198436379432678,
      "learning_rate": 0.00017894137734775185,
      "loss": 1.8975,
      "step": 185
    },
    {
      "epoch": 0.10810810810810811,
      "grad_norm": 0.5192174911499023,
      "learning_rate": 0.0001783722253841776,
      "loss": 1.9407,
      "step": 190
    },
    {
      "epoch": 0.11095305832147938,
      "grad_norm": 0.5123754739761353,
      "learning_rate": 0.00017780307342060332,
      "loss": 1.7721,
      "step": 195
    },
    {
      "epoch": 0.11379800853485064,
      "grad_norm": 0.5444467067718506,
      "learning_rate": 0.00017723392145702904,
      "loss": 1.7668,
      "step": 200
    },
    {
      "epoch": 0.1166429587482219,
      "grad_norm": 0.48558250069618225,
      "learning_rate": 0.00017666476949345476,
      "loss": 1.8502,
      "step": 205
    },
    {
      "epoch": 0.11948790896159317,
      "grad_norm": 0.5235543847084045,
      "learning_rate": 0.00017609561752988048,
      "loss": 1.7303,
      "step": 210
    },
    {
      "epoch": 0.12233285917496443,
      "grad_norm": 0.5079710483551025,
      "learning_rate": 0.0001755264655663062,
      "loss": 1.6375,
      "step": 215
    },
    {
      "epoch": 0.1251778093883357,
      "grad_norm": 0.5100827813148499,
      "learning_rate": 0.00017495731360273194,
      "loss": 1.7996,
      "step": 220
    },
    {
      "epoch": 0.12802275960170698,
      "grad_norm": 0.524852991104126,
      "learning_rate": 0.00017438816163915766,
      "loss": 1.7877,
      "step": 225
    },
    {
      "epoch": 0.13086770981507823,
      "grad_norm": 0.5219963788986206,
      "learning_rate": 0.0001738190096755834,
      "loss": 1.8222,
      "step": 230
    },
    {
      "epoch": 0.1337126600284495,
      "grad_norm": 0.54615318775177,
      "learning_rate": 0.00017324985771200913,
      "loss": 1.8048,
      "step": 235
    },
    {
      "epoch": 0.13655761024182078,
      "grad_norm": 0.4962216913700104,
      "learning_rate": 0.00017268070574843485,
      "loss": 1.7844,
      "step": 240
    },
    {
      "epoch": 0.13940256045519203,
      "grad_norm": 0.5254645943641663,
      "learning_rate": 0.00017211155378486057,
      "loss": 1.806,
      "step": 245
    },
    {
      "epoch": 0.1422475106685633,
      "grad_norm": 0.5344192981719971,
      "learning_rate": 0.0001715424018212863,
      "loss": 1.7977,
      "step": 250
    },
    {
      "epoch": 0.14509246088193456,
      "grad_norm": 0.5260816216468811,
      "learning_rate": 0.000170973249857712,
      "loss": 1.7991,
      "step": 255
    },
    {
      "epoch": 0.14793741109530584,
      "grad_norm": 0.536462664604187,
      "learning_rate": 0.00017040409789413773,
      "loss": 1.8164,
      "step": 260
    },
    {
      "epoch": 0.1507823613086771,
      "grad_norm": 0.534024715423584,
      "learning_rate": 0.00016983494593056345,
      "loss": 1.8092,
      "step": 265
    },
    {
      "epoch": 0.15362731152204837,
      "grad_norm": 0.5141861438751221,
      "learning_rate": 0.00016926579396698917,
      "loss": 1.8453,
      "step": 270
    },
    {
      "epoch": 0.15647226173541964,
      "grad_norm": 0.5123191475868225,
      "learning_rate": 0.00016869664200341492,
      "loss": 1.8227,
      "step": 275
    },
    {
      "epoch": 0.1593172119487909,
      "grad_norm": 0.5313526391983032,
      "learning_rate": 0.00016812749003984064,
      "loss": 1.7419,
      "step": 280
    },
    {
      "epoch": 0.16216216216216217,
      "grad_norm": 0.5069981813430786,
      "learning_rate": 0.0001675583380762664,
      "loss": 1.8631,
      "step": 285
    },
    {
      "epoch": 0.16500711237553342,
      "grad_norm": 0.5520071387290955,
      "learning_rate": 0.0001669891861126921,
      "loss": 1.8749,
      "step": 290
    },
    {
      "epoch": 0.1678520625889047,
      "grad_norm": 0.5548297166824341,
      "learning_rate": 0.00016642003414911783,
      "loss": 1.8134,
      "step": 295
    },
    {
      "epoch": 0.17069701280227595,
      "grad_norm": 0.5602995157241821,
      "learning_rate": 0.00016585088218554355,
      "loss": 1.8562,
      "step": 300
    },
    {
      "epoch": 0.17354196301564723,
      "grad_norm": 0.584997832775116,
      "learning_rate": 0.00016528173022196927,
      "loss": 1.6884,
      "step": 305
    },
    {
      "epoch": 0.1763869132290185,
      "grad_norm": 0.5142406821250916,
      "learning_rate": 0.000164712578258395,
      "loss": 1.7798,
      "step": 310
    },
    {
      "epoch": 0.17923186344238975,
      "grad_norm": 0.542140543460846,
      "learning_rate": 0.00016414342629482074,
      "loss": 1.8115,
      "step": 315
    },
    {
      "epoch": 0.18207681365576103,
      "grad_norm": 0.5546067953109741,
      "learning_rate": 0.00016357427433124646,
      "loss": 1.8028,
      "step": 320
    },
    {
      "epoch": 0.18492176386913228,
      "grad_norm": 0.5099424719810486,
      "learning_rate": 0.00016300512236767218,
      "loss": 1.7713,
      "step": 325
    },
    {
      "epoch": 0.18776671408250356,
      "grad_norm": 0.5469363927841187,
      "learning_rate": 0.0001624359704040979,
      "loss": 1.7708,
      "step": 330
    },
    {
      "epoch": 0.1906116642958748,
      "grad_norm": 0.5607678890228271,
      "learning_rate": 0.00016186681844052362,
      "loss": 1.854,
      "step": 335
    },
    {
      "epoch": 0.1934566145092461,
      "grad_norm": 0.550093412399292,
      "learning_rate": 0.00016129766647694934,
      "loss": 1.7278,
      "step": 340
    },
    {
      "epoch": 0.19630156472261737,
      "grad_norm": 0.6458361744880676,
      "learning_rate": 0.00016072851451337508,
      "loss": 1.8848,
      "step": 345
    },
    {
      "epoch": 0.19914651493598862,
      "grad_norm": 0.5263909101486206,
      "learning_rate": 0.0001601593625498008,
      "loss": 1.7014,
      "step": 350
    },
    {
      "epoch": 0.2019914651493599,
      "grad_norm": 0.5069615244865417,
      "learning_rate": 0.00015959021058622655,
      "loss": 1.7338,
      "step": 355
    },
    {
      "epoch": 0.20483641536273114,
      "grad_norm": 0.5174620151519775,
      "learning_rate": 0.00015902105862265227,
      "loss": 1.711,
      "step": 360
    },
    {
      "epoch": 0.20768136557610242,
      "grad_norm": 0.536195695400238,
      "learning_rate": 0.000158451906659078,
      "loss": 1.8283,
      "step": 365
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 0.5349156856536865,
      "learning_rate": 0.0001578827546955037,
      "loss": 1.8303,
      "step": 370
    },
    {
      "epoch": 0.21337126600284495,
      "grad_norm": 0.5570517778396606,
      "learning_rate": 0.00015731360273192943,
      "loss": 1.6737,
      "step": 375
    },
    {
      "epoch": 0.21621621621621623,
      "grad_norm": 0.5562851428985596,
      "learning_rate": 0.00015674445076835515,
      "loss": 1.7031,
      "step": 380
    },
    {
      "epoch": 0.21906116642958748,
      "grad_norm": 0.5085716843605042,
      "learning_rate": 0.00015617529880478087,
      "loss": 1.7839,
      "step": 385
    },
    {
      "epoch": 0.22190611664295876,
      "grad_norm": 0.5509480834007263,
      "learning_rate": 0.0001556061468412066,
      "loss": 1.8322,
      "step": 390
    },
    {
      "epoch": 0.22475106685633,
      "grad_norm": 0.552862286567688,
      "learning_rate": 0.00015503699487763231,
      "loss": 1.8205,
      "step": 395
    },
    {
      "epoch": 0.22759601706970128,
      "grad_norm": 0.5689870119094849,
      "learning_rate": 0.00015446784291405806,
      "loss": 1.8252,
      "step": 400
    },
    {
      "epoch": 0.23044096728307253,
      "grad_norm": 0.5379787683486938,
      "learning_rate": 0.00015389869095048378,
      "loss": 1.7698,
      "step": 405
    },
    {
      "epoch": 0.2332859174964438,
      "grad_norm": 0.5686361193656921,
      "learning_rate": 0.00015332953898690953,
      "loss": 1.81,
      "step": 410
    },
    {
      "epoch": 0.2361308677098151,
      "grad_norm": 0.5446853041648865,
      "learning_rate": 0.00015276038702333525,
      "loss": 1.741,
      "step": 415
    },
    {
      "epoch": 0.23897581792318634,
      "grad_norm": 0.549289882183075,
      "learning_rate": 0.00015219123505976097,
      "loss": 1.7816,
      "step": 420
    },
    {
      "epoch": 0.24182076813655762,
      "grad_norm": 0.5685895085334778,
      "learning_rate": 0.0001516220830961867,
      "loss": 1.7425,
      "step": 425
    },
    {
      "epoch": 0.24466571834992887,
      "grad_norm": 0.5328943729400635,
      "learning_rate": 0.0001510529311326124,
      "loss": 1.7754,
      "step": 430
    },
    {
      "epoch": 0.24751066856330015,
      "grad_norm": 0.5207207202911377,
      "learning_rate": 0.00015048377916903813,
      "loss": 1.7066,
      "step": 435
    },
    {
      "epoch": 0.2503556187766714,
      "grad_norm": 0.5533854961395264,
      "learning_rate": 0.00014991462720546388,
      "loss": 1.8044,
      "step": 440
    },
    {
      "epoch": 0.2532005689900427,
      "grad_norm": 0.5449127554893494,
      "learning_rate": 0.0001493454752418896,
      "loss": 1.7322,
      "step": 445
    },
    {
      "epoch": 0.25604551920341395,
      "grad_norm": 0.5602892637252808,
      "learning_rate": 0.00014877632327831532,
      "loss": 1.8496,
      "step": 450
    },
    {
      "epoch": 0.25889046941678523,
      "grad_norm": 0.6014001965522766,
      "learning_rate": 0.00014820717131474104,
      "loss": 1.7352,
      "step": 455
    },
    {
      "epoch": 0.26173541963015645,
      "grad_norm": 0.5059030652046204,
      "learning_rate": 0.00014763801935116676,
      "loss": 1.7903,
      "step": 460
    },
    {
      "epoch": 0.26458036984352773,
      "grad_norm": 0.5593153834342957,
      "learning_rate": 0.00014706886738759248,
      "loss": 1.8549,
      "step": 465
    },
    {
      "epoch": 0.267425320056899,
      "grad_norm": 0.5455724000930786,
      "learning_rate": 0.00014649971542401823,
      "loss": 1.8233,
      "step": 470
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 0.5374266505241394,
      "learning_rate": 0.00014593056346044395,
      "loss": 1.7071,
      "step": 475
    },
    {
      "epoch": 0.27311522048364156,
      "grad_norm": 0.5489568114280701,
      "learning_rate": 0.0001453614114968697,
      "loss": 1.7843,
      "step": 480
    },
    {
      "epoch": 0.2759601706970128,
      "grad_norm": 0.5015942454338074,
      "learning_rate": 0.00014479225953329541,
      "loss": 1.8701,
      "step": 485
    },
    {
      "epoch": 0.27880512091038406,
      "grad_norm": 0.5308811068534851,
      "learning_rate": 0.00014422310756972113,
      "loss": 1.8204,
      "step": 490
    },
    {
      "epoch": 0.28165007112375534,
      "grad_norm": 0.5483800768852234,
      "learning_rate": 0.00014365395560614685,
      "loss": 1.7381,
      "step": 495
    },
    {
      "epoch": 0.2844950213371266,
      "grad_norm": 0.541336715221405,
      "learning_rate": 0.00014308480364257257,
      "loss": 1.7185,
      "step": 500
    },
    {
      "epoch": 0.28733997155049784,
      "grad_norm": 0.56029212474823,
      "learning_rate": 0.0001425156516789983,
      "loss": 1.7201,
      "step": 505
    },
    {
      "epoch": 0.2901849217638691,
      "grad_norm": 0.5436985492706299,
      "learning_rate": 0.00014194649971542401,
      "loss": 1.8067,
      "step": 510
    },
    {
      "epoch": 0.2930298719772404,
      "grad_norm": 0.5450965762138367,
      "learning_rate": 0.00014137734775184974,
      "loss": 1.8599,
      "step": 515
    },
    {
      "epoch": 0.2958748221906117,
      "grad_norm": 0.5704220533370972,
      "learning_rate": 0.00014080819578827546,
      "loss": 1.7425,
      "step": 520
    },
    {
      "epoch": 0.29871977240398295,
      "grad_norm": 0.5314556360244751,
      "learning_rate": 0.0001402390438247012,
      "loss": 1.779,
      "step": 525
    },
    {
      "epoch": 0.3015647226173542,
      "grad_norm": 0.5982683897018433,
      "learning_rate": 0.00013966989186112692,
      "loss": 1.8211,
      "step": 530
    },
    {
      "epoch": 0.30440967283072545,
      "grad_norm": 0.5527154207229614,
      "learning_rate": 0.00013910073989755264,
      "loss": 1.7719,
      "step": 535
    },
    {
      "epoch": 0.30725462304409673,
      "grad_norm": 0.5959969162940979,
      "learning_rate": 0.0001385315879339784,
      "loss": 1.8675,
      "step": 540
    },
    {
      "epoch": 0.310099573257468,
      "grad_norm": 0.5495197176933289,
      "learning_rate": 0.0001379624359704041,
      "loss": 1.7607,
      "step": 545
    },
    {
      "epoch": 0.3129445234708393,
      "grad_norm": 0.5293635129928589,
      "learning_rate": 0.00013739328400682983,
      "loss": 1.664,
      "step": 550
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 0.5742693543434143,
      "learning_rate": 0.00013682413204325555,
      "loss": 1.7203,
      "step": 555
    },
    {
      "epoch": 0.3186344238975818,
      "grad_norm": 0.5422243475914001,
      "learning_rate": 0.00013625498007968127,
      "loss": 1.7767,
      "step": 560
    },
    {
      "epoch": 0.32147937411095306,
      "grad_norm": 0.5649269223213196,
      "learning_rate": 0.00013568582811610702,
      "loss": 1.8364,
      "step": 565
    },
    {
      "epoch": 0.32432432432432434,
      "grad_norm": 0.5209592580795288,
      "learning_rate": 0.00013511667615253274,
      "loss": 1.7459,
      "step": 570
    },
    {
      "epoch": 0.32716927453769556,
      "grad_norm": 0.5393198132514954,
      "learning_rate": 0.00013454752418895846,
      "loss": 1.7314,
      "step": 575
    },
    {
      "epoch": 0.33001422475106684,
      "grad_norm": 0.5036423802375793,
      "learning_rate": 0.00013397837222538418,
      "loss": 1.7298,
      "step": 580
    },
    {
      "epoch": 0.3328591749644381,
      "grad_norm": 0.5882377028465271,
      "learning_rate": 0.0001334092202618099,
      "loss": 1.7044,
      "step": 585
    },
    {
      "epoch": 0.3357041251778094,
      "grad_norm": 0.5572733879089355,
      "learning_rate": 0.00013284006829823562,
      "loss": 1.7219,
      "step": 590
    },
    {
      "epoch": 0.3385490753911807,
      "grad_norm": 0.5847784876823425,
      "learning_rate": 0.00013227091633466137,
      "loss": 1.8744,
      "step": 595
    },
    {
      "epoch": 0.3413940256045519,
      "grad_norm": 0.5702312588691711,
      "learning_rate": 0.0001317017643710871,
      "loss": 1.7204,
      "step": 600
    },
    {
      "epoch": 0.3442389758179232,
      "grad_norm": 0.5564872026443481,
      "learning_rate": 0.00013113261240751283,
      "loss": 1.7727,
      "step": 605
    },
    {
      "epoch": 0.34708392603129445,
      "grad_norm": 0.540156900882721,
      "learning_rate": 0.00013056346044393855,
      "loss": 1.7603,
      "step": 610
    },
    {
      "epoch": 0.34992887624466573,
      "grad_norm": 0.5652338862419128,
      "learning_rate": 0.00012999430848036428,
      "loss": 1.7419,
      "step": 615
    },
    {
      "epoch": 0.352773826458037,
      "grad_norm": 0.5541778206825256,
      "learning_rate": 0.00012942515651679,
      "loss": 1.7433,
      "step": 620
    },
    {
      "epoch": 0.35561877667140823,
      "grad_norm": 0.6089360117912292,
      "learning_rate": 0.00012885600455321572,
      "loss": 1.7459,
      "step": 625
    },
    {
      "epoch": 0.3584637268847795,
      "grad_norm": 0.5532107353210449,
      "learning_rate": 0.00012828685258964144,
      "loss": 1.7636,
      "step": 630
    },
    {
      "epoch": 0.3613086770981508,
      "grad_norm": 0.5951501131057739,
      "learning_rate": 0.00012771770062606716,
      "loss": 1.7711,
      "step": 635
    },
    {
      "epoch": 0.36415362731152207,
      "grad_norm": 0.530169665813446,
      "learning_rate": 0.00012714854866249288,
      "loss": 1.7152,
      "step": 640
    },
    {
      "epoch": 0.3669985775248933,
      "grad_norm": 0.539699137210846,
      "learning_rate": 0.0001265793966989186,
      "loss": 1.7072,
      "step": 645
    },
    {
      "epoch": 0.36984352773826457,
      "grad_norm": 0.5301237106323242,
      "learning_rate": 0.00012601024473534434,
      "loss": 1.7513,
      "step": 650
    },
    {
      "epoch": 0.37268847795163584,
      "grad_norm": 0.5763750076293945,
      "learning_rate": 0.00012544109277177006,
      "loss": 1.793,
      "step": 655
    },
    {
      "epoch": 0.3755334281650071,
      "grad_norm": 0.5494654774665833,
      "learning_rate": 0.00012487194080819578,
      "loss": 1.6776,
      "step": 660
    },
    {
      "epoch": 0.3783783783783784,
      "grad_norm": 0.5408105850219727,
      "learning_rate": 0.00012430278884462153,
      "loss": 1.7445,
      "step": 665
    },
    {
      "epoch": 0.3812233285917496,
      "grad_norm": 0.6453489065170288,
      "learning_rate": 0.00012373363688104725,
      "loss": 1.7409,
      "step": 670
    },
    {
      "epoch": 0.3840682788051209,
      "grad_norm": 0.5595253109931946,
      "learning_rate": 0.00012316448491747297,
      "loss": 1.6965,
      "step": 675
    },
    {
      "epoch": 0.3869132290184922,
      "grad_norm": 0.564319372177124,
      "learning_rate": 0.0001225953329538987,
      "loss": 1.7338,
      "step": 680
    },
    {
      "epoch": 0.38975817923186346,
      "grad_norm": 0.5440504550933838,
      "learning_rate": 0.00012202618099032441,
      "loss": 1.9428,
      "step": 685
    },
    {
      "epoch": 0.39260312944523473,
      "grad_norm": 0.6019558310508728,
      "learning_rate": 0.00012145702902675016,
      "loss": 1.8047,
      "step": 690
    },
    {
      "epoch": 0.39544807965860596,
      "grad_norm": 0.5315166711807251,
      "learning_rate": 0.00012088787706317588,
      "loss": 1.7543,
      "step": 695
    },
    {
      "epoch": 0.39829302987197723,
      "grad_norm": 0.5731531977653503,
      "learning_rate": 0.0001203187250996016,
      "loss": 1.7612,
      "step": 700
    },
    {
      "epoch": 0.4011379800853485,
      "grad_norm": 0.5607674717903137,
      "learning_rate": 0.00011974957313602732,
      "loss": 1.5737,
      "step": 705
    },
    {
      "epoch": 0.4039829302987198,
      "grad_norm": 0.5875958800315857,
      "learning_rate": 0.00011918042117245305,
      "loss": 1.7778,
      "step": 710
    },
    {
      "epoch": 0.406827880512091,
      "grad_norm": 0.5785971283912659,
      "learning_rate": 0.00011861126920887877,
      "loss": 1.8312,
      "step": 715
    },
    {
      "epoch": 0.4096728307254623,
      "grad_norm": 0.5557567477226257,
      "learning_rate": 0.0001180421172453045,
      "loss": 1.7575,
      "step": 720
    },
    {
      "epoch": 0.41251778093883357,
      "grad_norm": 0.5731320381164551,
      "learning_rate": 0.00011747296528173021,
      "loss": 1.6867,
      "step": 725
    },
    {
      "epoch": 0.41536273115220484,
      "grad_norm": 0.5766249299049377,
      "learning_rate": 0.00011690381331815596,
      "loss": 1.854,
      "step": 730
    },
    {
      "epoch": 0.4182076813655761,
      "grad_norm": 0.5616649985313416,
      "learning_rate": 0.00011633466135458168,
      "loss": 1.7036,
      "step": 735
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 0.5768249034881592,
      "learning_rate": 0.00011576550939100742,
      "loss": 1.7663,
      "step": 740
    },
    {
      "epoch": 0.4238975817923186,
      "grad_norm": 0.5535956025123596,
      "learning_rate": 0.00011519635742743314,
      "loss": 1.6985,
      "step": 745
    },
    {
      "epoch": 0.4267425320056899,
      "grad_norm": 0.6212028861045837,
      "learning_rate": 0.00011462720546385886,
      "loss": 1.7025,
      "step": 750
    },
    {
      "epoch": 0.4295874822190612,
      "grad_norm": 0.5723338723182678,
      "learning_rate": 0.00011405805350028458,
      "loss": 1.6444,
      "step": 755
    },
    {
      "epoch": 0.43243243243243246,
      "grad_norm": 0.611225426197052,
      "learning_rate": 0.0001134889015367103,
      "loss": 1.7765,
      "step": 760
    },
    {
      "epoch": 0.4352773826458037,
      "grad_norm": 0.566222608089447,
      "learning_rate": 0.00011291974957313603,
      "loss": 1.733,
      "step": 765
    },
    {
      "epoch": 0.43812233285917496,
      "grad_norm": 0.6035428643226624,
      "learning_rate": 0.00011235059760956175,
      "loss": 1.7163,
      "step": 770
    },
    {
      "epoch": 0.44096728307254623,
      "grad_norm": 0.5727686285972595,
      "learning_rate": 0.0001117814456459875,
      "loss": 1.7746,
      "step": 775
    },
    {
      "epoch": 0.4438122332859175,
      "grad_norm": 0.6219565272331238,
      "learning_rate": 0.00011121229368241322,
      "loss": 1.8464,
      "step": 780
    },
    {
      "epoch": 0.4466571834992888,
      "grad_norm": 0.5219248533248901,
      "learning_rate": 0.00011064314171883894,
      "loss": 1.7434,
      "step": 785
    },
    {
      "epoch": 0.44950213371266,
      "grad_norm": 0.582021176815033,
      "learning_rate": 0.00011007398975526466,
      "loss": 1.721,
      "step": 790
    },
    {
      "epoch": 0.4523470839260313,
      "grad_norm": 0.5642722845077515,
      "learning_rate": 0.00010950483779169038,
      "loss": 1.7102,
      "step": 795
    },
    {
      "epoch": 0.45519203413940257,
      "grad_norm": 0.5546287894248962,
      "learning_rate": 0.00010893568582811611,
      "loss": 1.7288,
      "step": 800
    },
    {
      "epoch": 0.45803698435277385,
      "grad_norm": 0.5949670076370239,
      "learning_rate": 0.00010836653386454183,
      "loss": 1.6573,
      "step": 805
    },
    {
      "epoch": 0.46088193456614507,
      "grad_norm": 0.5746669173240662,
      "learning_rate": 0.00010779738190096755,
      "loss": 1.6618,
      "step": 810
    },
    {
      "epoch": 0.46372688477951635,
      "grad_norm": 0.5729794502258301,
      "learning_rate": 0.0001072282299373933,
      "loss": 1.8412,
      "step": 815
    },
    {
      "epoch": 0.4665718349928876,
      "grad_norm": 0.5517909526824951,
      "learning_rate": 0.00010665907797381902,
      "loss": 1.8665,
      "step": 820
    },
    {
      "epoch": 0.4694167852062589,
      "grad_norm": 0.5782808661460876,
      "learning_rate": 0.00010608992601024474,
      "loss": 1.7508,
      "step": 825
    },
    {
      "epoch": 0.4722617354196302,
      "grad_norm": 0.5499626994132996,
      "learning_rate": 0.00010552077404667046,
      "loss": 1.6991,
      "step": 830
    },
    {
      "epoch": 0.4751066856330014,
      "grad_norm": 0.61516934633255,
      "learning_rate": 0.0001049516220830962,
      "loss": 1.8193,
      "step": 835
    },
    {
      "epoch": 0.4779516358463727,
      "grad_norm": 0.5890849828720093,
      "learning_rate": 0.00010438247011952192,
      "loss": 1.7798,
      "step": 840
    },
    {
      "epoch": 0.48079658605974396,
      "grad_norm": 0.583966851234436,
      "learning_rate": 0.00010381331815594764,
      "loss": 1.8858,
      "step": 845
    },
    {
      "epoch": 0.48364153627311524,
      "grad_norm": 0.5633518099784851,
      "learning_rate": 0.00010324416619237336,
      "loss": 1.6819,
      "step": 850
    },
    {
      "epoch": 0.4864864864864865,
      "grad_norm": 0.5884907841682434,
      "learning_rate": 0.0001026750142287991,
      "loss": 1.7301,
      "step": 855
    },
    {
      "epoch": 0.48933143669985774,
      "grad_norm": 0.5635905265808105,
      "learning_rate": 0.00010210586226522482,
      "loss": 1.6595,
      "step": 860
    },
    {
      "epoch": 0.492176386913229,
      "grad_norm": 0.6026513576507568,
      "learning_rate": 0.00010153671030165054,
      "loss": 1.6527,
      "step": 865
    },
    {
      "epoch": 0.4950213371266003,
      "grad_norm": 0.5916198492050171,
      "learning_rate": 0.00010096755833807628,
      "loss": 1.6546,
      "step": 870
    },
    {
      "epoch": 0.49786628733997157,
      "grad_norm": 0.5900142192840576,
      "learning_rate": 0.000100398406374502,
      "loss": 1.7762,
      "step": 875
    },
    {
      "epoch": 0.5007112375533428,
      "grad_norm": 0.5492935180664062,
      "learning_rate": 9.982925441092772e-05,
      "loss": 1.7851,
      "step": 880
    },
    {
      "epoch": 0.5035561877667141,
      "grad_norm": 0.5612053871154785,
      "learning_rate": 9.926010244735345e-05,
      "loss": 1.7455,
      "step": 885
    },
    {
      "epoch": 0.5064011379800853,
      "grad_norm": 0.5632994174957275,
      "learning_rate": 9.869095048377917e-05,
      "loss": 1.8396,
      "step": 890
    },
    {
      "epoch": 0.5092460881934566,
      "grad_norm": 0.5755983591079712,
      "learning_rate": 9.81217985202049e-05,
      "loss": 1.6753,
      "step": 895
    },
    {
      "epoch": 0.5120910384068279,
      "grad_norm": 0.7010552883148193,
      "learning_rate": 9.755264655663063e-05,
      "loss": 1.9816,
      "step": 900
    },
    {
      "epoch": 0.5149359886201992,
      "grad_norm": 0.5746469497680664,
      "learning_rate": 9.698349459305635e-05,
      "loss": 1.7013,
      "step": 905
    },
    {
      "epoch": 0.5177809388335705,
      "grad_norm": 0.5224916934967041,
      "learning_rate": 9.641434262948208e-05,
      "loss": 1.7116,
      "step": 910
    },
    {
      "epoch": 0.5206258890469416,
      "grad_norm": 0.5763961672782898,
      "learning_rate": 9.58451906659078e-05,
      "loss": 1.8295,
      "step": 915
    },
    {
      "epoch": 0.5234708392603129,
      "grad_norm": 0.6124321222305298,
      "learning_rate": 9.527603870233352e-05,
      "loss": 1.7434,
      "step": 920
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 0.5676684975624084,
      "learning_rate": 9.470688673875925e-05,
      "loss": 1.8323,
      "step": 925
    },
    {
      "epoch": 0.5291607396870555,
      "grad_norm": 0.5371504426002502,
      "learning_rate": 9.413773477518499e-05,
      "loss": 1.7473,
      "step": 930
    },
    {
      "epoch": 0.5320056899004267,
      "grad_norm": 0.5374114513397217,
      "learning_rate": 9.356858281161071e-05,
      "loss": 1.7573,
      "step": 935
    },
    {
      "epoch": 0.534850640113798,
      "grad_norm": 0.5383320450782776,
      "learning_rate": 9.299943084803643e-05,
      "loss": 1.8232,
      "step": 940
    },
    {
      "epoch": 0.5376955903271693,
      "grad_norm": 0.5817322134971619,
      "learning_rate": 9.243027888446215e-05,
      "loss": 1.7043,
      "step": 945
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 0.604423463344574,
      "learning_rate": 9.186112692088788e-05,
      "loss": 1.8469,
      "step": 950
    },
    {
      "epoch": 0.5433854907539118,
      "grad_norm": 0.5531787276268005,
      "learning_rate": 9.12919749573136e-05,
      "loss": 1.7224,
      "step": 955
    },
    {
      "epoch": 0.5462304409672831,
      "grad_norm": 0.6045641899108887,
      "learning_rate": 9.072282299373934e-05,
      "loss": 1.7781,
      "step": 960
    },
    {
      "epoch": 0.5490753911806543,
      "grad_norm": 0.5772977471351624,
      "learning_rate": 9.015367103016506e-05,
      "loss": 1.7165,
      "step": 965
    },
    {
      "epoch": 0.5519203413940256,
      "grad_norm": 0.6719211935997009,
      "learning_rate": 8.958451906659079e-05,
      "loss": 1.7178,
      "step": 970
    },
    {
      "epoch": 0.5547652916073968,
      "grad_norm": 0.5559521317481995,
      "learning_rate": 8.901536710301651e-05,
      "loss": 1.6589,
      "step": 975
    },
    {
      "epoch": 0.5576102418207681,
      "grad_norm": 0.5972409844398499,
      "learning_rate": 8.844621513944223e-05,
      "loss": 1.6577,
      "step": 980
    },
    {
      "epoch": 0.5604551920341394,
      "grad_norm": 0.6995476484298706,
      "learning_rate": 8.787706317586795e-05,
      "loss": 1.7441,
      "step": 985
    },
    {
      "epoch": 0.5633001422475107,
      "grad_norm": 0.5824615955352783,
      "learning_rate": 8.730791121229369e-05,
      "loss": 1.7179,
      "step": 990
    },
    {
      "epoch": 0.566145092460882,
      "grad_norm": 0.5460747480392456,
      "learning_rate": 8.673875924871942e-05,
      "loss": 1.6482,
      "step": 995
    },
    {
      "epoch": 0.5689900426742532,
      "grad_norm": 0.5684509873390198,
      "learning_rate": 8.616960728514514e-05,
      "loss": 1.7218,
      "step": 1000
    },
    {
      "epoch": 0.5718349928876245,
      "grad_norm": 0.6217830777168274,
      "learning_rate": 8.560045532157086e-05,
      "loss": 1.6805,
      "step": 1005
    },
    {
      "epoch": 0.5746799431009957,
      "grad_norm": 0.5829430818557739,
      "learning_rate": 8.503130335799659e-05,
      "loss": 1.7212,
      "step": 1010
    },
    {
      "epoch": 0.577524893314367,
      "grad_norm": 0.6019183397293091,
      "learning_rate": 8.446215139442231e-05,
      "loss": 1.6955,
      "step": 1015
    },
    {
      "epoch": 0.5803698435277382,
      "grad_norm": 0.5685757994651794,
      "learning_rate": 8.389299943084805e-05,
      "loss": 1.688,
      "step": 1020
    },
    {
      "epoch": 0.5832147937411095,
      "grad_norm": 0.6473103761672974,
      "learning_rate": 8.332384746727377e-05,
      "loss": 1.7721,
      "step": 1025
    },
    {
      "epoch": 0.5860597439544808,
      "grad_norm": 0.5764181017875671,
      "learning_rate": 8.275469550369949e-05,
      "loss": 1.7335,
      "step": 1030
    },
    {
      "epoch": 0.5889046941678521,
      "grad_norm": 0.6085451245307922,
      "learning_rate": 8.218554354012522e-05,
      "loss": 1.7596,
      "step": 1035
    },
    {
      "epoch": 0.5917496443812233,
      "grad_norm": 0.6124461889266968,
      "learning_rate": 8.161639157655094e-05,
      "loss": 1.6852,
      "step": 1040
    },
    {
      "epoch": 0.5945945945945946,
      "grad_norm": 0.5921880006790161,
      "learning_rate": 8.104723961297666e-05,
      "loss": 1.7117,
      "step": 1045
    },
    {
      "epoch": 0.5974395448079659,
      "grad_norm": 0.5981326699256897,
      "learning_rate": 8.04780876494024e-05,
      "loss": 1.7984,
      "step": 1050
    },
    {
      "epoch": 0.6002844950213371,
      "grad_norm": 0.5426524877548218,
      "learning_rate": 7.990893568582813e-05,
      "loss": 1.8123,
      "step": 1055
    },
    {
      "epoch": 0.6031294452347084,
      "grad_norm": 0.5638580918312073,
      "learning_rate": 7.933978372225385e-05,
      "loss": 1.7472,
      "step": 1060
    },
    {
      "epoch": 0.6059743954480796,
      "grad_norm": 0.570369303226471,
      "learning_rate": 7.877063175867957e-05,
      "loss": 1.7568,
      "step": 1065
    },
    {
      "epoch": 0.6088193456614509,
      "grad_norm": 0.6893725395202637,
      "learning_rate": 7.820147979510529e-05,
      "loss": 1.7695,
      "step": 1070
    },
    {
      "epoch": 0.6116642958748222,
      "grad_norm": 0.5904287695884705,
      "learning_rate": 7.763232783153102e-05,
      "loss": 1.7861,
      "step": 1075
    },
    {
      "epoch": 0.6145092460881935,
      "grad_norm": 0.5804920792579651,
      "learning_rate": 7.706317586795674e-05,
      "loss": 1.66,
      "step": 1080
    },
    {
      "epoch": 0.6173541963015647,
      "grad_norm": 0.6381001472473145,
      "learning_rate": 7.649402390438248e-05,
      "loss": 1.684,
      "step": 1085
    },
    {
      "epoch": 0.620199146514936,
      "grad_norm": 0.5892635583877563,
      "learning_rate": 7.59248719408082e-05,
      "loss": 1.6972,
      "step": 1090
    },
    {
      "epoch": 0.6230440967283073,
      "grad_norm": 0.6379626989364624,
      "learning_rate": 7.535571997723393e-05,
      "loss": 1.6848,
      "step": 1095
    },
    {
      "epoch": 0.6258890469416786,
      "grad_norm": 0.572711706161499,
      "learning_rate": 7.478656801365965e-05,
      "loss": 1.7298,
      "step": 1100
    },
    {
      "epoch": 0.6287339971550497,
      "grad_norm": 0.5847349166870117,
      "learning_rate": 7.421741605008537e-05,
      "loss": 1.7819,
      "step": 1105
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 0.6167197823524475,
      "learning_rate": 7.364826408651109e-05,
      "loss": 1.7664,
      "step": 1110
    },
    {
      "epoch": 0.6344238975817923,
      "grad_norm": 0.6279488801956177,
      "learning_rate": 7.307911212293683e-05,
      "loss": 1.7964,
      "step": 1115
    },
    {
      "epoch": 0.6372688477951636,
      "grad_norm": 0.6007311344146729,
      "learning_rate": 7.250996015936256e-05,
      "loss": 1.7813,
      "step": 1120
    },
    {
      "epoch": 0.6401137980085349,
      "grad_norm": 0.5748448967933655,
      "learning_rate": 7.194080819578828e-05,
      "loss": 1.6972,
      "step": 1125
    },
    {
      "epoch": 0.6429587482219061,
      "grad_norm": 0.5873613357543945,
      "learning_rate": 7.1371656232214e-05,
      "loss": 1.8296,
      "step": 1130
    },
    {
      "epoch": 0.6458036984352774,
      "grad_norm": 0.6300820112228394,
      "learning_rate": 7.080250426863973e-05,
      "loss": 1.8217,
      "step": 1135
    },
    {
      "epoch": 0.6486486486486487,
      "grad_norm": 0.5689193606376648,
      "learning_rate": 7.023335230506545e-05,
      "loss": 1.7111,
      "step": 1140
    },
    {
      "epoch": 0.65149359886202,
      "grad_norm": 0.5765858292579651,
      "learning_rate": 6.966420034149117e-05,
      "loss": 1.767,
      "step": 1145
    },
    {
      "epoch": 0.6543385490753911,
      "grad_norm": 0.6325014233589172,
      "learning_rate": 6.909504837791691e-05,
      "loss": 1.7518,
      "step": 1150
    },
    {
      "epoch": 0.6571834992887624,
      "grad_norm": 0.5864705443382263,
      "learning_rate": 6.852589641434263e-05,
      "loss": 1.6642,
      "step": 1155
    },
    {
      "epoch": 0.6600284495021337,
      "grad_norm": 0.6529000997543335,
      "learning_rate": 6.795674445076836e-05,
      "loss": 1.7201,
      "step": 1160
    },
    {
      "epoch": 0.662873399715505,
      "grad_norm": 0.583341658115387,
      "learning_rate": 6.738759248719408e-05,
      "loss": 1.8098,
      "step": 1165
    },
    {
      "epoch": 0.6657183499288762,
      "grad_norm": 0.5808201432228088,
      "learning_rate": 6.68184405236198e-05,
      "loss": 1.7568,
      "step": 1170
    },
    {
      "epoch": 0.6685633001422475,
      "grad_norm": 0.6283050179481506,
      "learning_rate": 6.624928856004554e-05,
      "loss": 1.7158,
      "step": 1175
    },
    {
      "epoch": 0.6714082503556188,
      "grad_norm": 0.649728000164032,
      "learning_rate": 6.568013659647127e-05,
      "loss": 1.6192,
      "step": 1180
    },
    {
      "epoch": 0.6742532005689901,
      "grad_norm": 0.6067515015602112,
      "learning_rate": 6.511098463289699e-05,
      "loss": 1.606,
      "step": 1185
    },
    {
      "epoch": 0.6770981507823614,
      "grad_norm": 0.6094313263893127,
      "learning_rate": 6.454183266932271e-05,
      "loss": 1.6755,
      "step": 1190
    },
    {
      "epoch": 0.6799431009957326,
      "grad_norm": 0.6243488788604736,
      "learning_rate": 6.397268070574843e-05,
      "loss": 1.7009,
      "step": 1195
    },
    {
      "epoch": 0.6827880512091038,
      "grad_norm": 0.6204275488853455,
      "learning_rate": 6.340352874217417e-05,
      "loss": 1.6668,
      "step": 1200
    },
    {
      "epoch": 0.6856330014224751,
      "grad_norm": 0.5630740523338318,
      "learning_rate": 6.283437677859989e-05,
      "loss": 1.6843,
      "step": 1205
    },
    {
      "epoch": 0.6884779516358464,
      "grad_norm": 0.5723122358322144,
      "learning_rate": 6.226522481502562e-05,
      "loss": 1.7045,
      "step": 1210
    },
    {
      "epoch": 0.6913229018492176,
      "grad_norm": 0.6597334742546082,
      "learning_rate": 6.169607285145134e-05,
      "loss": 1.7576,
      "step": 1215
    },
    {
      "epoch": 0.6941678520625889,
      "grad_norm": 0.6450833082199097,
      "learning_rate": 6.112692088787707e-05,
      "loss": 1.6931,
      "step": 1220
    },
    {
      "epoch": 0.6970128022759602,
      "grad_norm": 0.6136056184768677,
      "learning_rate": 6.055776892430279e-05,
      "loss": 1.7849,
      "step": 1225
    },
    {
      "epoch": 0.6998577524893315,
      "grad_norm": 0.6242358088493347,
      "learning_rate": 5.9988616960728513e-05,
      "loss": 1.8561,
      "step": 1230
    },
    {
      "epoch": 0.7027027027027027,
      "grad_norm": 0.5976651906967163,
      "learning_rate": 5.941946499715424e-05,
      "loss": 1.6939,
      "step": 1235
    },
    {
      "epoch": 0.705547652916074,
      "grad_norm": 0.5928819179534912,
      "learning_rate": 5.8850313033579974e-05,
      "loss": 1.6852,
      "step": 1240
    },
    {
      "epoch": 0.7083926031294452,
      "grad_norm": 0.5962180495262146,
      "learning_rate": 5.8281161070005694e-05,
      "loss": 1.7234,
      "step": 1245
    },
    {
      "epoch": 0.7112375533428165,
      "grad_norm": 0.6015336513519287,
      "learning_rate": 5.771200910643142e-05,
      "loss": 1.6799,
      "step": 1250
    }
  ],
  "logging_steps": 5,
  "max_steps": 1757,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.213794381748634e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
