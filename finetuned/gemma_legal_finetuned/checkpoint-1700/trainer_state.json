{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9672830725462305,
  "eval_steps": 500,
  "global_step": 1700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002844950213371266,
      "grad_norm": 0.4949305057525635,
      "learning_rate": 0.00019943084803642573,
      "loss": 2.2738,
      "step": 5
    },
    {
      "epoch": 0.005689900426742532,
      "grad_norm": 0.5478770136833191,
      "learning_rate": 0.00019886169607285145,
      "loss": 2.1523,
      "step": 10
    },
    {
      "epoch": 0.008534850640113799,
      "grad_norm": 0.4918110966682434,
      "learning_rate": 0.00019829254410927717,
      "loss": 2.0182,
      "step": 15
    },
    {
      "epoch": 0.011379800853485065,
      "grad_norm": 0.46867555379867554,
      "learning_rate": 0.0001977233921457029,
      "loss": 2.0836,
      "step": 20
    },
    {
      "epoch": 0.01422475106685633,
      "grad_norm": 0.4838064908981323,
      "learning_rate": 0.00019715424018212864,
      "loss": 1.8437,
      "step": 25
    },
    {
      "epoch": 0.017069701280227598,
      "grad_norm": 0.5012277364730835,
      "learning_rate": 0.00019658508821855436,
      "loss": 1.8759,
      "step": 30
    },
    {
      "epoch": 0.01991465149359886,
      "grad_norm": 0.44116437435150146,
      "learning_rate": 0.0001960159362549801,
      "loss": 1.9221,
      "step": 35
    },
    {
      "epoch": 0.02275960170697013,
      "grad_norm": 0.46326640248298645,
      "learning_rate": 0.00019544678429140583,
      "loss": 1.8934,
      "step": 40
    },
    {
      "epoch": 0.025604551920341393,
      "grad_norm": 0.4289257824420929,
      "learning_rate": 0.00019487763232783155,
      "loss": 1.8505,
      "step": 45
    },
    {
      "epoch": 0.02844950213371266,
      "grad_norm": 0.46900564432144165,
      "learning_rate": 0.00019430848036425727,
      "loss": 1.9045,
      "step": 50
    },
    {
      "epoch": 0.031294452347083924,
      "grad_norm": 0.5247617363929749,
      "learning_rate": 0.00019373932840068299,
      "loss": 1.8131,
      "step": 55
    },
    {
      "epoch": 0.034139402560455195,
      "grad_norm": 0.4680875837802887,
      "learning_rate": 0.0001931701764371087,
      "loss": 1.8481,
      "step": 60
    },
    {
      "epoch": 0.03698435277382646,
      "grad_norm": 0.523281455039978,
      "learning_rate": 0.00019260102447353445,
      "loss": 1.9826,
      "step": 65
    },
    {
      "epoch": 0.03982930298719772,
      "grad_norm": 0.4950197637081146,
      "learning_rate": 0.00019203187250996017,
      "loss": 1.8674,
      "step": 70
    },
    {
      "epoch": 0.04267425320056899,
      "grad_norm": 0.5106218457221985,
      "learning_rate": 0.0001914627205463859,
      "loss": 1.8541,
      "step": 75
    },
    {
      "epoch": 0.04551920341394026,
      "grad_norm": 0.46364477276802063,
      "learning_rate": 0.00019089356858281161,
      "loss": 1.8454,
      "step": 80
    },
    {
      "epoch": 0.04836415362731152,
      "grad_norm": 0.5279164910316467,
      "learning_rate": 0.00019032441661923733,
      "loss": 1.7972,
      "step": 85
    },
    {
      "epoch": 0.051209103840682786,
      "grad_norm": 0.48782220482826233,
      "learning_rate": 0.00018975526465566305,
      "loss": 1.8047,
      "step": 90
    },
    {
      "epoch": 0.05405405405405406,
      "grad_norm": 0.5064043402671814,
      "learning_rate": 0.0001891861126920888,
      "loss": 1.7437,
      "step": 95
    },
    {
      "epoch": 0.05689900426742532,
      "grad_norm": 0.5325073003768921,
      "learning_rate": 0.00018861696072851452,
      "loss": 1.8588,
      "step": 100
    },
    {
      "epoch": 0.059743954480796585,
      "grad_norm": 0.4949420392513275,
      "learning_rate": 0.00018804780876494027,
      "loss": 1.9295,
      "step": 105
    },
    {
      "epoch": 0.06258890469416785,
      "grad_norm": 0.5514007806777954,
      "learning_rate": 0.000187478656801366,
      "loss": 1.8105,
      "step": 110
    },
    {
      "epoch": 0.06543385490753911,
      "grad_norm": 0.561600387096405,
      "learning_rate": 0.0001869095048377917,
      "loss": 1.8116,
      "step": 115
    },
    {
      "epoch": 0.06827880512091039,
      "grad_norm": 0.5257200598716736,
      "learning_rate": 0.00018634035287421743,
      "loss": 1.852,
      "step": 120
    },
    {
      "epoch": 0.07112375533428165,
      "grad_norm": 0.4855252802371979,
      "learning_rate": 0.00018577120091064315,
      "loss": 1.8639,
      "step": 125
    },
    {
      "epoch": 0.07396870554765292,
      "grad_norm": 0.5099745988845825,
      "learning_rate": 0.00018520204894706887,
      "loss": 1.7894,
      "step": 130
    },
    {
      "epoch": 0.07681365576102418,
      "grad_norm": 0.4951542317867279,
      "learning_rate": 0.0001846328969834946,
      "loss": 1.7587,
      "step": 135
    },
    {
      "epoch": 0.07965860597439545,
      "grad_norm": 0.5255674719810486,
      "learning_rate": 0.0001840637450199203,
      "loss": 1.9911,
      "step": 140
    },
    {
      "epoch": 0.08250355618776671,
      "grad_norm": 0.4962505102157593,
      "learning_rate": 0.00018349459305634603,
      "loss": 1.7364,
      "step": 145
    },
    {
      "epoch": 0.08534850640113797,
      "grad_norm": 0.49747973680496216,
      "learning_rate": 0.00018292544109277178,
      "loss": 1.8365,
      "step": 150
    },
    {
      "epoch": 0.08819345661450925,
      "grad_norm": 0.5000907182693481,
      "learning_rate": 0.0001823562891291975,
      "loss": 1.7884,
      "step": 155
    },
    {
      "epoch": 0.09103840682788052,
      "grad_norm": 0.5320129990577698,
      "learning_rate": 0.00018178713716562325,
      "loss": 1.8548,
      "step": 160
    },
    {
      "epoch": 0.09388335704125178,
      "grad_norm": 0.5494191646575928,
      "learning_rate": 0.00018121798520204897,
      "loss": 1.8061,
      "step": 165
    },
    {
      "epoch": 0.09672830725462304,
      "grad_norm": 0.5762655138969421,
      "learning_rate": 0.0001806488332384747,
      "loss": 1.8744,
      "step": 170
    },
    {
      "epoch": 0.09957325746799431,
      "grad_norm": 0.5443115830421448,
      "learning_rate": 0.0001800796812749004,
      "loss": 1.9422,
      "step": 175
    },
    {
      "epoch": 0.10241820768136557,
      "grad_norm": 0.5426173210144043,
      "learning_rate": 0.00017951052931132613,
      "loss": 1.9794,
      "step": 180
    },
    {
      "epoch": 0.10526315789473684,
      "grad_norm": 0.5198436379432678,
      "learning_rate": 0.00017894137734775185,
      "loss": 1.8975,
      "step": 185
    },
    {
      "epoch": 0.10810810810810811,
      "grad_norm": 0.5192174911499023,
      "learning_rate": 0.0001783722253841776,
      "loss": 1.9407,
      "step": 190
    },
    {
      "epoch": 0.11095305832147938,
      "grad_norm": 0.5123754739761353,
      "learning_rate": 0.00017780307342060332,
      "loss": 1.7721,
      "step": 195
    },
    {
      "epoch": 0.11379800853485064,
      "grad_norm": 0.5444467067718506,
      "learning_rate": 0.00017723392145702904,
      "loss": 1.7668,
      "step": 200
    },
    {
      "epoch": 0.1166429587482219,
      "grad_norm": 0.48558250069618225,
      "learning_rate": 0.00017666476949345476,
      "loss": 1.8502,
      "step": 205
    },
    {
      "epoch": 0.11948790896159317,
      "grad_norm": 0.5235543847084045,
      "learning_rate": 0.00017609561752988048,
      "loss": 1.7303,
      "step": 210
    },
    {
      "epoch": 0.12233285917496443,
      "grad_norm": 0.5079710483551025,
      "learning_rate": 0.0001755264655663062,
      "loss": 1.6375,
      "step": 215
    },
    {
      "epoch": 0.1251778093883357,
      "grad_norm": 0.5100827813148499,
      "learning_rate": 0.00017495731360273194,
      "loss": 1.7996,
      "step": 220
    },
    {
      "epoch": 0.12802275960170698,
      "grad_norm": 0.524852991104126,
      "learning_rate": 0.00017438816163915766,
      "loss": 1.7877,
      "step": 225
    },
    {
      "epoch": 0.13086770981507823,
      "grad_norm": 0.5219963788986206,
      "learning_rate": 0.0001738190096755834,
      "loss": 1.8222,
      "step": 230
    },
    {
      "epoch": 0.1337126600284495,
      "grad_norm": 0.54615318775177,
      "learning_rate": 0.00017324985771200913,
      "loss": 1.8048,
      "step": 235
    },
    {
      "epoch": 0.13655761024182078,
      "grad_norm": 0.4962216913700104,
      "learning_rate": 0.00017268070574843485,
      "loss": 1.7844,
      "step": 240
    },
    {
      "epoch": 0.13940256045519203,
      "grad_norm": 0.5254645943641663,
      "learning_rate": 0.00017211155378486057,
      "loss": 1.806,
      "step": 245
    },
    {
      "epoch": 0.1422475106685633,
      "grad_norm": 0.5344192981719971,
      "learning_rate": 0.0001715424018212863,
      "loss": 1.7977,
      "step": 250
    },
    {
      "epoch": 0.14509246088193456,
      "grad_norm": 0.5260816216468811,
      "learning_rate": 0.000170973249857712,
      "loss": 1.7991,
      "step": 255
    },
    {
      "epoch": 0.14793741109530584,
      "grad_norm": 0.536462664604187,
      "learning_rate": 0.00017040409789413773,
      "loss": 1.8164,
      "step": 260
    },
    {
      "epoch": 0.1507823613086771,
      "grad_norm": 0.534024715423584,
      "learning_rate": 0.00016983494593056345,
      "loss": 1.8092,
      "step": 265
    },
    {
      "epoch": 0.15362731152204837,
      "grad_norm": 0.5141861438751221,
      "learning_rate": 0.00016926579396698917,
      "loss": 1.8453,
      "step": 270
    },
    {
      "epoch": 0.15647226173541964,
      "grad_norm": 0.5123191475868225,
      "learning_rate": 0.00016869664200341492,
      "loss": 1.8227,
      "step": 275
    },
    {
      "epoch": 0.1593172119487909,
      "grad_norm": 0.5313526391983032,
      "learning_rate": 0.00016812749003984064,
      "loss": 1.7419,
      "step": 280
    },
    {
      "epoch": 0.16216216216216217,
      "grad_norm": 0.5069981813430786,
      "learning_rate": 0.0001675583380762664,
      "loss": 1.8631,
      "step": 285
    },
    {
      "epoch": 0.16500711237553342,
      "grad_norm": 0.5520071387290955,
      "learning_rate": 0.0001669891861126921,
      "loss": 1.8749,
      "step": 290
    },
    {
      "epoch": 0.1678520625889047,
      "grad_norm": 0.5548297166824341,
      "learning_rate": 0.00016642003414911783,
      "loss": 1.8134,
      "step": 295
    },
    {
      "epoch": 0.17069701280227595,
      "grad_norm": 0.5602995157241821,
      "learning_rate": 0.00016585088218554355,
      "loss": 1.8562,
      "step": 300
    },
    {
      "epoch": 0.17354196301564723,
      "grad_norm": 0.584997832775116,
      "learning_rate": 0.00016528173022196927,
      "loss": 1.6884,
      "step": 305
    },
    {
      "epoch": 0.1763869132290185,
      "grad_norm": 0.5142406821250916,
      "learning_rate": 0.000164712578258395,
      "loss": 1.7798,
      "step": 310
    },
    {
      "epoch": 0.17923186344238975,
      "grad_norm": 0.542140543460846,
      "learning_rate": 0.00016414342629482074,
      "loss": 1.8115,
      "step": 315
    },
    {
      "epoch": 0.18207681365576103,
      "grad_norm": 0.5546067953109741,
      "learning_rate": 0.00016357427433124646,
      "loss": 1.8028,
      "step": 320
    },
    {
      "epoch": 0.18492176386913228,
      "grad_norm": 0.5099424719810486,
      "learning_rate": 0.00016300512236767218,
      "loss": 1.7713,
      "step": 325
    },
    {
      "epoch": 0.18776671408250356,
      "grad_norm": 0.5469363927841187,
      "learning_rate": 0.0001624359704040979,
      "loss": 1.7708,
      "step": 330
    },
    {
      "epoch": 0.1906116642958748,
      "grad_norm": 0.5607678890228271,
      "learning_rate": 0.00016186681844052362,
      "loss": 1.854,
      "step": 335
    },
    {
      "epoch": 0.1934566145092461,
      "grad_norm": 0.550093412399292,
      "learning_rate": 0.00016129766647694934,
      "loss": 1.7278,
      "step": 340
    },
    {
      "epoch": 0.19630156472261737,
      "grad_norm": 0.6458361744880676,
      "learning_rate": 0.00016072851451337508,
      "loss": 1.8848,
      "step": 345
    },
    {
      "epoch": 0.19914651493598862,
      "grad_norm": 0.5263909101486206,
      "learning_rate": 0.0001601593625498008,
      "loss": 1.7014,
      "step": 350
    },
    {
      "epoch": 0.2019914651493599,
      "grad_norm": 0.5069615244865417,
      "learning_rate": 0.00015959021058622655,
      "loss": 1.7338,
      "step": 355
    },
    {
      "epoch": 0.20483641536273114,
      "grad_norm": 0.5174620151519775,
      "learning_rate": 0.00015902105862265227,
      "loss": 1.711,
      "step": 360
    },
    {
      "epoch": 0.20768136557610242,
      "grad_norm": 0.536195695400238,
      "learning_rate": 0.000158451906659078,
      "loss": 1.8283,
      "step": 365
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 0.5349156856536865,
      "learning_rate": 0.0001578827546955037,
      "loss": 1.8303,
      "step": 370
    },
    {
      "epoch": 0.21337126600284495,
      "grad_norm": 0.5570517778396606,
      "learning_rate": 0.00015731360273192943,
      "loss": 1.6737,
      "step": 375
    },
    {
      "epoch": 0.21621621621621623,
      "grad_norm": 0.5562851428985596,
      "learning_rate": 0.00015674445076835515,
      "loss": 1.7031,
      "step": 380
    },
    {
      "epoch": 0.21906116642958748,
      "grad_norm": 0.5085716843605042,
      "learning_rate": 0.00015617529880478087,
      "loss": 1.7839,
      "step": 385
    },
    {
      "epoch": 0.22190611664295876,
      "grad_norm": 0.5509480834007263,
      "learning_rate": 0.0001556061468412066,
      "loss": 1.8322,
      "step": 390
    },
    {
      "epoch": 0.22475106685633,
      "grad_norm": 0.552862286567688,
      "learning_rate": 0.00015503699487763231,
      "loss": 1.8205,
      "step": 395
    },
    {
      "epoch": 0.22759601706970128,
      "grad_norm": 0.5689870119094849,
      "learning_rate": 0.00015446784291405806,
      "loss": 1.8252,
      "step": 400
    },
    {
      "epoch": 0.23044096728307253,
      "grad_norm": 0.5379787683486938,
      "learning_rate": 0.00015389869095048378,
      "loss": 1.7698,
      "step": 405
    },
    {
      "epoch": 0.2332859174964438,
      "grad_norm": 0.5686361193656921,
      "learning_rate": 0.00015332953898690953,
      "loss": 1.81,
      "step": 410
    },
    {
      "epoch": 0.2361308677098151,
      "grad_norm": 0.5446853041648865,
      "learning_rate": 0.00015276038702333525,
      "loss": 1.741,
      "step": 415
    },
    {
      "epoch": 0.23897581792318634,
      "grad_norm": 0.549289882183075,
      "learning_rate": 0.00015219123505976097,
      "loss": 1.7816,
      "step": 420
    },
    {
      "epoch": 0.24182076813655762,
      "grad_norm": 0.5685895085334778,
      "learning_rate": 0.0001516220830961867,
      "loss": 1.7425,
      "step": 425
    },
    {
      "epoch": 0.24466571834992887,
      "grad_norm": 0.5328943729400635,
      "learning_rate": 0.0001510529311326124,
      "loss": 1.7754,
      "step": 430
    },
    {
      "epoch": 0.24751066856330015,
      "grad_norm": 0.5207207202911377,
      "learning_rate": 0.00015048377916903813,
      "loss": 1.7066,
      "step": 435
    },
    {
      "epoch": 0.2503556187766714,
      "grad_norm": 0.5533854961395264,
      "learning_rate": 0.00014991462720546388,
      "loss": 1.8044,
      "step": 440
    },
    {
      "epoch": 0.2532005689900427,
      "grad_norm": 0.5449127554893494,
      "learning_rate": 0.0001493454752418896,
      "loss": 1.7322,
      "step": 445
    },
    {
      "epoch": 0.25604551920341395,
      "grad_norm": 0.5602892637252808,
      "learning_rate": 0.00014877632327831532,
      "loss": 1.8496,
      "step": 450
    },
    {
      "epoch": 0.25889046941678523,
      "grad_norm": 0.6014001965522766,
      "learning_rate": 0.00014820717131474104,
      "loss": 1.7352,
      "step": 455
    },
    {
      "epoch": 0.26173541963015645,
      "grad_norm": 0.5059030652046204,
      "learning_rate": 0.00014763801935116676,
      "loss": 1.7903,
      "step": 460
    },
    {
      "epoch": 0.26458036984352773,
      "grad_norm": 0.5593153834342957,
      "learning_rate": 0.00014706886738759248,
      "loss": 1.8549,
      "step": 465
    },
    {
      "epoch": 0.267425320056899,
      "grad_norm": 0.5455724000930786,
      "learning_rate": 0.00014649971542401823,
      "loss": 1.8233,
      "step": 470
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 0.5374266505241394,
      "learning_rate": 0.00014593056346044395,
      "loss": 1.7071,
      "step": 475
    },
    {
      "epoch": 0.27311522048364156,
      "grad_norm": 0.5489568114280701,
      "learning_rate": 0.0001453614114968697,
      "loss": 1.7843,
      "step": 480
    },
    {
      "epoch": 0.2759601706970128,
      "grad_norm": 0.5015942454338074,
      "learning_rate": 0.00014479225953329541,
      "loss": 1.8701,
      "step": 485
    },
    {
      "epoch": 0.27880512091038406,
      "grad_norm": 0.5308811068534851,
      "learning_rate": 0.00014422310756972113,
      "loss": 1.8204,
      "step": 490
    },
    {
      "epoch": 0.28165007112375534,
      "grad_norm": 0.5483800768852234,
      "learning_rate": 0.00014365395560614685,
      "loss": 1.7381,
      "step": 495
    },
    {
      "epoch": 0.2844950213371266,
      "grad_norm": 0.541336715221405,
      "learning_rate": 0.00014308480364257257,
      "loss": 1.7185,
      "step": 500
    },
    {
      "epoch": 0.28733997155049784,
      "grad_norm": 0.56029212474823,
      "learning_rate": 0.0001425156516789983,
      "loss": 1.7201,
      "step": 505
    },
    {
      "epoch": 0.2901849217638691,
      "grad_norm": 0.5436985492706299,
      "learning_rate": 0.00014194649971542401,
      "loss": 1.8067,
      "step": 510
    },
    {
      "epoch": 0.2930298719772404,
      "grad_norm": 0.5450965762138367,
      "learning_rate": 0.00014137734775184974,
      "loss": 1.8599,
      "step": 515
    },
    {
      "epoch": 0.2958748221906117,
      "grad_norm": 0.5704220533370972,
      "learning_rate": 0.00014080819578827546,
      "loss": 1.7425,
      "step": 520
    },
    {
      "epoch": 0.29871977240398295,
      "grad_norm": 0.5314556360244751,
      "learning_rate": 0.0001402390438247012,
      "loss": 1.779,
      "step": 525
    },
    {
      "epoch": 0.3015647226173542,
      "grad_norm": 0.5982683897018433,
      "learning_rate": 0.00013966989186112692,
      "loss": 1.8211,
      "step": 530
    },
    {
      "epoch": 0.30440967283072545,
      "grad_norm": 0.5527154207229614,
      "learning_rate": 0.00013910073989755264,
      "loss": 1.7719,
      "step": 535
    },
    {
      "epoch": 0.30725462304409673,
      "grad_norm": 0.5959969162940979,
      "learning_rate": 0.0001385315879339784,
      "loss": 1.8675,
      "step": 540
    },
    {
      "epoch": 0.310099573257468,
      "grad_norm": 0.5495197176933289,
      "learning_rate": 0.0001379624359704041,
      "loss": 1.7607,
      "step": 545
    },
    {
      "epoch": 0.3129445234708393,
      "grad_norm": 0.5293635129928589,
      "learning_rate": 0.00013739328400682983,
      "loss": 1.664,
      "step": 550
    },
    {
      "epoch": 0.3157894736842105,
      "grad_norm": 0.5742693543434143,
      "learning_rate": 0.00013682413204325555,
      "loss": 1.7203,
      "step": 555
    },
    {
      "epoch": 0.3186344238975818,
      "grad_norm": 0.5422243475914001,
      "learning_rate": 0.00013625498007968127,
      "loss": 1.7767,
      "step": 560
    },
    {
      "epoch": 0.32147937411095306,
      "grad_norm": 0.5649269223213196,
      "learning_rate": 0.00013568582811610702,
      "loss": 1.8364,
      "step": 565
    },
    {
      "epoch": 0.32432432432432434,
      "grad_norm": 0.5209592580795288,
      "learning_rate": 0.00013511667615253274,
      "loss": 1.7459,
      "step": 570
    },
    {
      "epoch": 0.32716927453769556,
      "grad_norm": 0.5393198132514954,
      "learning_rate": 0.00013454752418895846,
      "loss": 1.7314,
      "step": 575
    },
    {
      "epoch": 0.33001422475106684,
      "grad_norm": 0.5036423802375793,
      "learning_rate": 0.00013397837222538418,
      "loss": 1.7298,
      "step": 580
    },
    {
      "epoch": 0.3328591749644381,
      "grad_norm": 0.5882377028465271,
      "learning_rate": 0.0001334092202618099,
      "loss": 1.7044,
      "step": 585
    },
    {
      "epoch": 0.3357041251778094,
      "grad_norm": 0.5572733879089355,
      "learning_rate": 0.00013284006829823562,
      "loss": 1.7219,
      "step": 590
    },
    {
      "epoch": 0.3385490753911807,
      "grad_norm": 0.5847784876823425,
      "learning_rate": 0.00013227091633466137,
      "loss": 1.8744,
      "step": 595
    },
    {
      "epoch": 0.3413940256045519,
      "grad_norm": 0.5702312588691711,
      "learning_rate": 0.0001317017643710871,
      "loss": 1.7204,
      "step": 600
    },
    {
      "epoch": 0.3442389758179232,
      "grad_norm": 0.5564872026443481,
      "learning_rate": 0.00013113261240751283,
      "loss": 1.7727,
      "step": 605
    },
    {
      "epoch": 0.34708392603129445,
      "grad_norm": 0.540156900882721,
      "learning_rate": 0.00013056346044393855,
      "loss": 1.7603,
      "step": 610
    },
    {
      "epoch": 0.34992887624466573,
      "grad_norm": 0.5652338862419128,
      "learning_rate": 0.00012999430848036428,
      "loss": 1.7419,
      "step": 615
    },
    {
      "epoch": 0.352773826458037,
      "grad_norm": 0.5541778206825256,
      "learning_rate": 0.00012942515651679,
      "loss": 1.7433,
      "step": 620
    },
    {
      "epoch": 0.35561877667140823,
      "grad_norm": 0.6089360117912292,
      "learning_rate": 0.00012885600455321572,
      "loss": 1.7459,
      "step": 625
    },
    {
      "epoch": 0.3584637268847795,
      "grad_norm": 0.5532107353210449,
      "learning_rate": 0.00012828685258964144,
      "loss": 1.7636,
      "step": 630
    },
    {
      "epoch": 0.3613086770981508,
      "grad_norm": 0.5951501131057739,
      "learning_rate": 0.00012771770062606716,
      "loss": 1.7711,
      "step": 635
    },
    {
      "epoch": 0.36415362731152207,
      "grad_norm": 0.530169665813446,
      "learning_rate": 0.00012714854866249288,
      "loss": 1.7152,
      "step": 640
    },
    {
      "epoch": 0.3669985775248933,
      "grad_norm": 0.539699137210846,
      "learning_rate": 0.0001265793966989186,
      "loss": 1.7072,
      "step": 645
    },
    {
      "epoch": 0.36984352773826457,
      "grad_norm": 0.5301237106323242,
      "learning_rate": 0.00012601024473534434,
      "loss": 1.7513,
      "step": 650
    },
    {
      "epoch": 0.37268847795163584,
      "grad_norm": 0.5763750076293945,
      "learning_rate": 0.00012544109277177006,
      "loss": 1.793,
      "step": 655
    },
    {
      "epoch": 0.3755334281650071,
      "grad_norm": 0.5494654774665833,
      "learning_rate": 0.00012487194080819578,
      "loss": 1.6776,
      "step": 660
    },
    {
      "epoch": 0.3783783783783784,
      "grad_norm": 0.5408105850219727,
      "learning_rate": 0.00012430278884462153,
      "loss": 1.7445,
      "step": 665
    },
    {
      "epoch": 0.3812233285917496,
      "grad_norm": 0.6453489065170288,
      "learning_rate": 0.00012373363688104725,
      "loss": 1.7409,
      "step": 670
    },
    {
      "epoch": 0.3840682788051209,
      "grad_norm": 0.5595253109931946,
      "learning_rate": 0.00012316448491747297,
      "loss": 1.6965,
      "step": 675
    },
    {
      "epoch": 0.3869132290184922,
      "grad_norm": 0.564319372177124,
      "learning_rate": 0.0001225953329538987,
      "loss": 1.7338,
      "step": 680
    },
    {
      "epoch": 0.38975817923186346,
      "grad_norm": 0.5440504550933838,
      "learning_rate": 0.00012202618099032441,
      "loss": 1.9428,
      "step": 685
    },
    {
      "epoch": 0.39260312944523473,
      "grad_norm": 0.6019558310508728,
      "learning_rate": 0.00012145702902675016,
      "loss": 1.8047,
      "step": 690
    },
    {
      "epoch": 0.39544807965860596,
      "grad_norm": 0.5315166711807251,
      "learning_rate": 0.00012088787706317588,
      "loss": 1.7543,
      "step": 695
    },
    {
      "epoch": 0.39829302987197723,
      "grad_norm": 0.5731531977653503,
      "learning_rate": 0.0001203187250996016,
      "loss": 1.7612,
      "step": 700
    },
    {
      "epoch": 0.4011379800853485,
      "grad_norm": 0.5607674717903137,
      "learning_rate": 0.00011974957313602732,
      "loss": 1.5737,
      "step": 705
    },
    {
      "epoch": 0.4039829302987198,
      "grad_norm": 0.5875958800315857,
      "learning_rate": 0.00011918042117245305,
      "loss": 1.7778,
      "step": 710
    },
    {
      "epoch": 0.406827880512091,
      "grad_norm": 0.5785971283912659,
      "learning_rate": 0.00011861126920887877,
      "loss": 1.8312,
      "step": 715
    },
    {
      "epoch": 0.4096728307254623,
      "grad_norm": 0.5557567477226257,
      "learning_rate": 0.0001180421172453045,
      "loss": 1.7575,
      "step": 720
    },
    {
      "epoch": 0.41251778093883357,
      "grad_norm": 0.5731320381164551,
      "learning_rate": 0.00011747296528173021,
      "loss": 1.6867,
      "step": 725
    },
    {
      "epoch": 0.41536273115220484,
      "grad_norm": 0.5766249299049377,
      "learning_rate": 0.00011690381331815596,
      "loss": 1.854,
      "step": 730
    },
    {
      "epoch": 0.4182076813655761,
      "grad_norm": 0.5616649985313416,
      "learning_rate": 0.00011633466135458168,
      "loss": 1.7036,
      "step": 735
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 0.5768249034881592,
      "learning_rate": 0.00011576550939100742,
      "loss": 1.7663,
      "step": 740
    },
    {
      "epoch": 0.4238975817923186,
      "grad_norm": 0.5535956025123596,
      "learning_rate": 0.00011519635742743314,
      "loss": 1.6985,
      "step": 745
    },
    {
      "epoch": 0.4267425320056899,
      "grad_norm": 0.6212028861045837,
      "learning_rate": 0.00011462720546385886,
      "loss": 1.7025,
      "step": 750
    },
    {
      "epoch": 0.4295874822190612,
      "grad_norm": 0.5723338723182678,
      "learning_rate": 0.00011405805350028458,
      "loss": 1.6444,
      "step": 755
    },
    {
      "epoch": 0.43243243243243246,
      "grad_norm": 0.611225426197052,
      "learning_rate": 0.0001134889015367103,
      "loss": 1.7765,
      "step": 760
    },
    {
      "epoch": 0.4352773826458037,
      "grad_norm": 0.566222608089447,
      "learning_rate": 0.00011291974957313603,
      "loss": 1.733,
      "step": 765
    },
    {
      "epoch": 0.43812233285917496,
      "grad_norm": 0.6035428643226624,
      "learning_rate": 0.00011235059760956175,
      "loss": 1.7163,
      "step": 770
    },
    {
      "epoch": 0.44096728307254623,
      "grad_norm": 0.5727686285972595,
      "learning_rate": 0.0001117814456459875,
      "loss": 1.7746,
      "step": 775
    },
    {
      "epoch": 0.4438122332859175,
      "grad_norm": 0.6219565272331238,
      "learning_rate": 0.00011121229368241322,
      "loss": 1.8464,
      "step": 780
    },
    {
      "epoch": 0.4466571834992888,
      "grad_norm": 0.5219248533248901,
      "learning_rate": 0.00011064314171883894,
      "loss": 1.7434,
      "step": 785
    },
    {
      "epoch": 0.44950213371266,
      "grad_norm": 0.582021176815033,
      "learning_rate": 0.00011007398975526466,
      "loss": 1.721,
      "step": 790
    },
    {
      "epoch": 0.4523470839260313,
      "grad_norm": 0.5642722845077515,
      "learning_rate": 0.00010950483779169038,
      "loss": 1.7102,
      "step": 795
    },
    {
      "epoch": 0.45519203413940257,
      "grad_norm": 0.5546287894248962,
      "learning_rate": 0.00010893568582811611,
      "loss": 1.7288,
      "step": 800
    },
    {
      "epoch": 0.45803698435277385,
      "grad_norm": 0.5949670076370239,
      "learning_rate": 0.00010836653386454183,
      "loss": 1.6573,
      "step": 805
    },
    {
      "epoch": 0.46088193456614507,
      "grad_norm": 0.5746669173240662,
      "learning_rate": 0.00010779738190096755,
      "loss": 1.6618,
      "step": 810
    },
    {
      "epoch": 0.46372688477951635,
      "grad_norm": 0.5729794502258301,
      "learning_rate": 0.0001072282299373933,
      "loss": 1.8412,
      "step": 815
    },
    {
      "epoch": 0.4665718349928876,
      "grad_norm": 0.5517909526824951,
      "learning_rate": 0.00010665907797381902,
      "loss": 1.8665,
      "step": 820
    },
    {
      "epoch": 0.4694167852062589,
      "grad_norm": 0.5782808661460876,
      "learning_rate": 0.00010608992601024474,
      "loss": 1.7508,
      "step": 825
    },
    {
      "epoch": 0.4722617354196302,
      "grad_norm": 0.5499626994132996,
      "learning_rate": 0.00010552077404667046,
      "loss": 1.6991,
      "step": 830
    },
    {
      "epoch": 0.4751066856330014,
      "grad_norm": 0.61516934633255,
      "learning_rate": 0.0001049516220830962,
      "loss": 1.8193,
      "step": 835
    },
    {
      "epoch": 0.4779516358463727,
      "grad_norm": 0.5890849828720093,
      "learning_rate": 0.00010438247011952192,
      "loss": 1.7798,
      "step": 840
    },
    {
      "epoch": 0.48079658605974396,
      "grad_norm": 0.583966851234436,
      "learning_rate": 0.00010381331815594764,
      "loss": 1.8858,
      "step": 845
    },
    {
      "epoch": 0.48364153627311524,
      "grad_norm": 0.5633518099784851,
      "learning_rate": 0.00010324416619237336,
      "loss": 1.6819,
      "step": 850
    },
    {
      "epoch": 0.4864864864864865,
      "grad_norm": 0.5884907841682434,
      "learning_rate": 0.0001026750142287991,
      "loss": 1.7301,
      "step": 855
    },
    {
      "epoch": 0.48933143669985774,
      "grad_norm": 0.5635905265808105,
      "learning_rate": 0.00010210586226522482,
      "loss": 1.6595,
      "step": 860
    },
    {
      "epoch": 0.492176386913229,
      "grad_norm": 0.6026513576507568,
      "learning_rate": 0.00010153671030165054,
      "loss": 1.6527,
      "step": 865
    },
    {
      "epoch": 0.4950213371266003,
      "grad_norm": 0.5916198492050171,
      "learning_rate": 0.00010096755833807628,
      "loss": 1.6546,
      "step": 870
    },
    {
      "epoch": 0.49786628733997157,
      "grad_norm": 0.5900142192840576,
      "learning_rate": 0.000100398406374502,
      "loss": 1.7762,
      "step": 875
    },
    {
      "epoch": 0.5007112375533428,
      "grad_norm": 0.5492935180664062,
      "learning_rate": 9.982925441092772e-05,
      "loss": 1.7851,
      "step": 880
    },
    {
      "epoch": 0.5035561877667141,
      "grad_norm": 0.5612053871154785,
      "learning_rate": 9.926010244735345e-05,
      "loss": 1.7455,
      "step": 885
    },
    {
      "epoch": 0.5064011379800853,
      "grad_norm": 0.5632994174957275,
      "learning_rate": 9.869095048377917e-05,
      "loss": 1.8396,
      "step": 890
    },
    {
      "epoch": 0.5092460881934566,
      "grad_norm": 0.5755983591079712,
      "learning_rate": 9.81217985202049e-05,
      "loss": 1.6753,
      "step": 895
    },
    {
      "epoch": 0.5120910384068279,
      "grad_norm": 0.7010552883148193,
      "learning_rate": 9.755264655663063e-05,
      "loss": 1.9816,
      "step": 900
    },
    {
      "epoch": 0.5149359886201992,
      "grad_norm": 0.5746469497680664,
      "learning_rate": 9.698349459305635e-05,
      "loss": 1.7013,
      "step": 905
    },
    {
      "epoch": 0.5177809388335705,
      "grad_norm": 0.5224916934967041,
      "learning_rate": 9.641434262948208e-05,
      "loss": 1.7116,
      "step": 910
    },
    {
      "epoch": 0.5206258890469416,
      "grad_norm": 0.5763961672782898,
      "learning_rate": 9.58451906659078e-05,
      "loss": 1.8295,
      "step": 915
    },
    {
      "epoch": 0.5234708392603129,
      "grad_norm": 0.6124321222305298,
      "learning_rate": 9.527603870233352e-05,
      "loss": 1.7434,
      "step": 920
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 0.5676684975624084,
      "learning_rate": 9.470688673875925e-05,
      "loss": 1.8323,
      "step": 925
    },
    {
      "epoch": 0.5291607396870555,
      "grad_norm": 0.5371504426002502,
      "learning_rate": 9.413773477518499e-05,
      "loss": 1.7473,
      "step": 930
    },
    {
      "epoch": 0.5320056899004267,
      "grad_norm": 0.5374114513397217,
      "learning_rate": 9.356858281161071e-05,
      "loss": 1.7573,
      "step": 935
    },
    {
      "epoch": 0.534850640113798,
      "grad_norm": 0.5383320450782776,
      "learning_rate": 9.299943084803643e-05,
      "loss": 1.8232,
      "step": 940
    },
    {
      "epoch": 0.5376955903271693,
      "grad_norm": 0.5817322134971619,
      "learning_rate": 9.243027888446215e-05,
      "loss": 1.7043,
      "step": 945
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 0.604423463344574,
      "learning_rate": 9.186112692088788e-05,
      "loss": 1.8469,
      "step": 950
    },
    {
      "epoch": 0.5433854907539118,
      "grad_norm": 0.5531787276268005,
      "learning_rate": 9.12919749573136e-05,
      "loss": 1.7224,
      "step": 955
    },
    {
      "epoch": 0.5462304409672831,
      "grad_norm": 0.6045641899108887,
      "learning_rate": 9.072282299373934e-05,
      "loss": 1.7781,
      "step": 960
    },
    {
      "epoch": 0.5490753911806543,
      "grad_norm": 0.5772977471351624,
      "learning_rate": 9.015367103016506e-05,
      "loss": 1.7165,
      "step": 965
    },
    {
      "epoch": 0.5519203413940256,
      "grad_norm": 0.6719211935997009,
      "learning_rate": 8.958451906659079e-05,
      "loss": 1.7178,
      "step": 970
    },
    {
      "epoch": 0.5547652916073968,
      "grad_norm": 0.5559521317481995,
      "learning_rate": 8.901536710301651e-05,
      "loss": 1.6589,
      "step": 975
    },
    {
      "epoch": 0.5576102418207681,
      "grad_norm": 0.5972409844398499,
      "learning_rate": 8.844621513944223e-05,
      "loss": 1.6577,
      "step": 980
    },
    {
      "epoch": 0.5604551920341394,
      "grad_norm": 0.6995476484298706,
      "learning_rate": 8.787706317586795e-05,
      "loss": 1.7441,
      "step": 985
    },
    {
      "epoch": 0.5633001422475107,
      "grad_norm": 0.5824615955352783,
      "learning_rate": 8.730791121229369e-05,
      "loss": 1.7179,
      "step": 990
    },
    {
      "epoch": 0.566145092460882,
      "grad_norm": 0.5460747480392456,
      "learning_rate": 8.673875924871942e-05,
      "loss": 1.6482,
      "step": 995
    },
    {
      "epoch": 0.5689900426742532,
      "grad_norm": 0.5684509873390198,
      "learning_rate": 8.616960728514514e-05,
      "loss": 1.7218,
      "step": 1000
    },
    {
      "epoch": 0.5718349928876245,
      "grad_norm": 0.6217830777168274,
      "learning_rate": 8.560045532157086e-05,
      "loss": 1.6805,
      "step": 1005
    },
    {
      "epoch": 0.5746799431009957,
      "grad_norm": 0.5829430818557739,
      "learning_rate": 8.503130335799659e-05,
      "loss": 1.7212,
      "step": 1010
    },
    {
      "epoch": 0.577524893314367,
      "grad_norm": 0.6019183397293091,
      "learning_rate": 8.446215139442231e-05,
      "loss": 1.6955,
      "step": 1015
    },
    {
      "epoch": 0.5803698435277382,
      "grad_norm": 0.5685757994651794,
      "learning_rate": 8.389299943084805e-05,
      "loss": 1.688,
      "step": 1020
    },
    {
      "epoch": 0.5832147937411095,
      "grad_norm": 0.6473103761672974,
      "learning_rate": 8.332384746727377e-05,
      "loss": 1.7721,
      "step": 1025
    },
    {
      "epoch": 0.5860597439544808,
      "grad_norm": 0.5764181017875671,
      "learning_rate": 8.275469550369949e-05,
      "loss": 1.7335,
      "step": 1030
    },
    {
      "epoch": 0.5889046941678521,
      "grad_norm": 0.6085451245307922,
      "learning_rate": 8.218554354012522e-05,
      "loss": 1.7596,
      "step": 1035
    },
    {
      "epoch": 0.5917496443812233,
      "grad_norm": 0.6124461889266968,
      "learning_rate": 8.161639157655094e-05,
      "loss": 1.6852,
      "step": 1040
    },
    {
      "epoch": 0.5945945945945946,
      "grad_norm": 0.5921880006790161,
      "learning_rate": 8.104723961297666e-05,
      "loss": 1.7117,
      "step": 1045
    },
    {
      "epoch": 0.5974395448079659,
      "grad_norm": 0.5981326699256897,
      "learning_rate": 8.04780876494024e-05,
      "loss": 1.7984,
      "step": 1050
    },
    {
      "epoch": 0.6002844950213371,
      "grad_norm": 0.5426524877548218,
      "learning_rate": 7.990893568582813e-05,
      "loss": 1.8123,
      "step": 1055
    },
    {
      "epoch": 0.6031294452347084,
      "grad_norm": 0.5638580918312073,
      "learning_rate": 7.933978372225385e-05,
      "loss": 1.7472,
      "step": 1060
    },
    {
      "epoch": 0.6059743954480796,
      "grad_norm": 0.570369303226471,
      "learning_rate": 7.877063175867957e-05,
      "loss": 1.7568,
      "step": 1065
    },
    {
      "epoch": 0.6088193456614509,
      "grad_norm": 0.6893725395202637,
      "learning_rate": 7.820147979510529e-05,
      "loss": 1.7695,
      "step": 1070
    },
    {
      "epoch": 0.6116642958748222,
      "grad_norm": 0.5904287695884705,
      "learning_rate": 7.763232783153102e-05,
      "loss": 1.7861,
      "step": 1075
    },
    {
      "epoch": 0.6145092460881935,
      "grad_norm": 0.5804920792579651,
      "learning_rate": 7.706317586795674e-05,
      "loss": 1.66,
      "step": 1080
    },
    {
      "epoch": 0.6173541963015647,
      "grad_norm": 0.6381001472473145,
      "learning_rate": 7.649402390438248e-05,
      "loss": 1.684,
      "step": 1085
    },
    {
      "epoch": 0.620199146514936,
      "grad_norm": 0.5892635583877563,
      "learning_rate": 7.59248719408082e-05,
      "loss": 1.6972,
      "step": 1090
    },
    {
      "epoch": 0.6230440967283073,
      "grad_norm": 0.6379626989364624,
      "learning_rate": 7.535571997723393e-05,
      "loss": 1.6848,
      "step": 1095
    },
    {
      "epoch": 0.6258890469416786,
      "grad_norm": 0.572711706161499,
      "learning_rate": 7.478656801365965e-05,
      "loss": 1.7298,
      "step": 1100
    },
    {
      "epoch": 0.6287339971550497,
      "grad_norm": 0.5847349166870117,
      "learning_rate": 7.421741605008537e-05,
      "loss": 1.7819,
      "step": 1105
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 0.6167197823524475,
      "learning_rate": 7.364826408651109e-05,
      "loss": 1.7664,
      "step": 1110
    },
    {
      "epoch": 0.6344238975817923,
      "grad_norm": 0.6279488801956177,
      "learning_rate": 7.307911212293683e-05,
      "loss": 1.7964,
      "step": 1115
    },
    {
      "epoch": 0.6372688477951636,
      "grad_norm": 0.6007311344146729,
      "learning_rate": 7.250996015936256e-05,
      "loss": 1.7813,
      "step": 1120
    },
    {
      "epoch": 0.6401137980085349,
      "grad_norm": 0.5748448967933655,
      "learning_rate": 7.194080819578828e-05,
      "loss": 1.6972,
      "step": 1125
    },
    {
      "epoch": 0.6429587482219061,
      "grad_norm": 0.5873613357543945,
      "learning_rate": 7.1371656232214e-05,
      "loss": 1.8296,
      "step": 1130
    },
    {
      "epoch": 0.6458036984352774,
      "grad_norm": 0.6300820112228394,
      "learning_rate": 7.080250426863973e-05,
      "loss": 1.8217,
      "step": 1135
    },
    {
      "epoch": 0.6486486486486487,
      "grad_norm": 0.5689193606376648,
      "learning_rate": 7.023335230506545e-05,
      "loss": 1.7111,
      "step": 1140
    },
    {
      "epoch": 0.65149359886202,
      "grad_norm": 0.5765858292579651,
      "learning_rate": 6.966420034149117e-05,
      "loss": 1.767,
      "step": 1145
    },
    {
      "epoch": 0.6543385490753911,
      "grad_norm": 0.6325014233589172,
      "learning_rate": 6.909504837791691e-05,
      "loss": 1.7518,
      "step": 1150
    },
    {
      "epoch": 0.6571834992887624,
      "grad_norm": 0.5864705443382263,
      "learning_rate": 6.852589641434263e-05,
      "loss": 1.6642,
      "step": 1155
    },
    {
      "epoch": 0.6600284495021337,
      "grad_norm": 0.6529000997543335,
      "learning_rate": 6.795674445076836e-05,
      "loss": 1.7201,
      "step": 1160
    },
    {
      "epoch": 0.662873399715505,
      "grad_norm": 0.583341658115387,
      "learning_rate": 6.738759248719408e-05,
      "loss": 1.8098,
      "step": 1165
    },
    {
      "epoch": 0.6657183499288762,
      "grad_norm": 0.5808201432228088,
      "learning_rate": 6.68184405236198e-05,
      "loss": 1.7568,
      "step": 1170
    },
    {
      "epoch": 0.6685633001422475,
      "grad_norm": 0.6283050179481506,
      "learning_rate": 6.624928856004554e-05,
      "loss": 1.7158,
      "step": 1175
    },
    {
      "epoch": 0.6714082503556188,
      "grad_norm": 0.649728000164032,
      "learning_rate": 6.568013659647127e-05,
      "loss": 1.6192,
      "step": 1180
    },
    {
      "epoch": 0.6742532005689901,
      "grad_norm": 0.6067515015602112,
      "learning_rate": 6.511098463289699e-05,
      "loss": 1.606,
      "step": 1185
    },
    {
      "epoch": 0.6770981507823614,
      "grad_norm": 0.6094313263893127,
      "learning_rate": 6.454183266932271e-05,
      "loss": 1.6755,
      "step": 1190
    },
    {
      "epoch": 0.6799431009957326,
      "grad_norm": 0.6243488788604736,
      "learning_rate": 6.397268070574843e-05,
      "loss": 1.7009,
      "step": 1195
    },
    {
      "epoch": 0.6827880512091038,
      "grad_norm": 0.6204275488853455,
      "learning_rate": 6.340352874217417e-05,
      "loss": 1.6668,
      "step": 1200
    },
    {
      "epoch": 0.6856330014224751,
      "grad_norm": 0.5630740523338318,
      "learning_rate": 6.283437677859989e-05,
      "loss": 1.6843,
      "step": 1205
    },
    {
      "epoch": 0.6884779516358464,
      "grad_norm": 0.5723122358322144,
      "learning_rate": 6.226522481502562e-05,
      "loss": 1.7045,
      "step": 1210
    },
    {
      "epoch": 0.6913229018492176,
      "grad_norm": 0.6597334742546082,
      "learning_rate": 6.169607285145134e-05,
      "loss": 1.7576,
      "step": 1215
    },
    {
      "epoch": 0.6941678520625889,
      "grad_norm": 0.6450833082199097,
      "learning_rate": 6.112692088787707e-05,
      "loss": 1.6931,
      "step": 1220
    },
    {
      "epoch": 0.6970128022759602,
      "grad_norm": 0.6136056184768677,
      "learning_rate": 6.055776892430279e-05,
      "loss": 1.7849,
      "step": 1225
    },
    {
      "epoch": 0.6998577524893315,
      "grad_norm": 0.6242358088493347,
      "learning_rate": 5.9988616960728513e-05,
      "loss": 1.8561,
      "step": 1230
    },
    {
      "epoch": 0.7027027027027027,
      "grad_norm": 0.5976651906967163,
      "learning_rate": 5.941946499715424e-05,
      "loss": 1.6939,
      "step": 1235
    },
    {
      "epoch": 0.705547652916074,
      "grad_norm": 0.5928819179534912,
      "learning_rate": 5.8850313033579974e-05,
      "loss": 1.6852,
      "step": 1240
    },
    {
      "epoch": 0.7083926031294452,
      "grad_norm": 0.5962180495262146,
      "learning_rate": 5.8281161070005694e-05,
      "loss": 1.7234,
      "step": 1245
    },
    {
      "epoch": 0.7112375533428165,
      "grad_norm": 0.6015336513519287,
      "learning_rate": 5.771200910643142e-05,
      "loss": 1.6799,
      "step": 1250
    },
    {
      "epoch": 0.7140825035561877,
      "grad_norm": 0.5861414074897766,
      "learning_rate": 5.714285714285714e-05,
      "loss": 1.7737,
      "step": 1255
    },
    {
      "epoch": 0.716927453769559,
      "grad_norm": 0.5443202257156372,
      "learning_rate": 5.6573705179282876e-05,
      "loss": 1.6755,
      "step": 1260
    },
    {
      "epoch": 0.7197724039829303,
      "grad_norm": 0.5904530882835388,
      "learning_rate": 5.60045532157086e-05,
      "loss": 1.6821,
      "step": 1265
    },
    {
      "epoch": 0.7226173541963016,
      "grad_norm": 0.6430090069770813,
      "learning_rate": 5.543540125213432e-05,
      "loss": 1.7762,
      "step": 1270
    },
    {
      "epoch": 0.7254623044096729,
      "grad_norm": 0.5764861106872559,
      "learning_rate": 5.486624928856004e-05,
      "loss": 1.7039,
      "step": 1275
    },
    {
      "epoch": 0.7283072546230441,
      "grad_norm": 0.6219922304153442,
      "learning_rate": 5.429709732498577e-05,
      "loss": 1.7636,
      "step": 1280
    },
    {
      "epoch": 0.7311522048364154,
      "grad_norm": 0.5936030149459839,
      "learning_rate": 5.3727945361411504e-05,
      "loss": 1.8162,
      "step": 1285
    },
    {
      "epoch": 0.7339971550497866,
      "grad_norm": 0.5911210775375366,
      "learning_rate": 5.3158793397837224e-05,
      "loss": 1.7889,
      "step": 1290
    },
    {
      "epoch": 0.7368421052631579,
      "grad_norm": 0.5804458856582642,
      "learning_rate": 5.258964143426295e-05,
      "loss": 1.7422,
      "step": 1295
    },
    {
      "epoch": 0.7396870554765291,
      "grad_norm": 0.565440833568573,
      "learning_rate": 5.202048947068867e-05,
      "loss": 1.7135,
      "step": 1300
    },
    {
      "epoch": 0.7425320056899004,
      "grad_norm": 0.5740842819213867,
      "learning_rate": 5.1451337507114405e-05,
      "loss": 1.6922,
      "step": 1305
    },
    {
      "epoch": 0.7453769559032717,
      "grad_norm": 0.5776804685592651,
      "learning_rate": 5.0882185543540125e-05,
      "loss": 1.7994,
      "step": 1310
    },
    {
      "epoch": 0.748221906116643,
      "grad_norm": 0.6261866092681885,
      "learning_rate": 5.031303357996585e-05,
      "loss": 1.9117,
      "step": 1315
    },
    {
      "epoch": 0.7510668563300142,
      "grad_norm": 0.6762325763702393,
      "learning_rate": 4.974388161639158e-05,
      "loss": 1.7302,
      "step": 1320
    },
    {
      "epoch": 0.7539118065433855,
      "grad_norm": 0.6117159128189087,
      "learning_rate": 4.9174729652817306e-05,
      "loss": 1.7194,
      "step": 1325
    },
    {
      "epoch": 0.7567567567567568,
      "grad_norm": 0.5898149609565735,
      "learning_rate": 4.860557768924303e-05,
      "loss": 1.5666,
      "step": 1330
    },
    {
      "epoch": 0.7596017069701281,
      "grad_norm": 0.5748351812362671,
      "learning_rate": 4.8036425725668753e-05,
      "loss": 1.7321,
      "step": 1335
    },
    {
      "epoch": 0.7624466571834992,
      "grad_norm": 0.6129909753799438,
      "learning_rate": 4.746727376209448e-05,
      "loss": 1.7216,
      "step": 1340
    },
    {
      "epoch": 0.7652916073968705,
      "grad_norm": 0.5599371194839478,
      "learning_rate": 4.689812179852021e-05,
      "loss": 1.7117,
      "step": 1345
    },
    {
      "epoch": 0.7681365576102418,
      "grad_norm": 0.5775051116943359,
      "learning_rate": 4.6328969834945934e-05,
      "loss": 1.6456,
      "step": 1350
    },
    {
      "epoch": 0.7709815078236131,
      "grad_norm": 0.5975096225738525,
      "learning_rate": 4.5759817871371655e-05,
      "loss": 1.7456,
      "step": 1355
    },
    {
      "epoch": 0.7738264580369844,
      "grad_norm": 0.5974733233451843,
      "learning_rate": 4.519066590779739e-05,
      "loss": 1.7209,
      "step": 1360
    },
    {
      "epoch": 0.7766714082503556,
      "grad_norm": 0.6598169803619385,
      "learning_rate": 4.462151394422311e-05,
      "loss": 1.798,
      "step": 1365
    },
    {
      "epoch": 0.7795163584637269,
      "grad_norm": 0.6017513275146484,
      "learning_rate": 4.4052361980648836e-05,
      "loss": 1.6888,
      "step": 1370
    },
    {
      "epoch": 0.7823613086770982,
      "grad_norm": 0.6057673096656799,
      "learning_rate": 4.348321001707456e-05,
      "loss": 1.7114,
      "step": 1375
    },
    {
      "epoch": 0.7852062588904695,
      "grad_norm": 0.6866137981414795,
      "learning_rate": 4.291405805350029e-05,
      "loss": 1.7969,
      "step": 1380
    },
    {
      "epoch": 0.7880512091038406,
      "grad_norm": 0.6028575897216797,
      "learning_rate": 4.234490608992601e-05,
      "loss": 1.7738,
      "step": 1385
    },
    {
      "epoch": 0.7908961593172119,
      "grad_norm": 0.5946513414382935,
      "learning_rate": 4.1775754126351744e-05,
      "loss": 1.7078,
      "step": 1390
    },
    {
      "epoch": 0.7937411095305832,
      "grad_norm": 0.5871905088424683,
      "learning_rate": 4.1206602162777464e-05,
      "loss": 1.6655,
      "step": 1395
    },
    {
      "epoch": 0.7965860597439545,
      "grad_norm": 0.6158661842346191,
      "learning_rate": 4.063745019920319e-05,
      "loss": 1.6948,
      "step": 1400
    },
    {
      "epoch": 0.7994310099573257,
      "grad_norm": 0.5818540453910828,
      "learning_rate": 4.006829823562892e-05,
      "loss": 1.7252,
      "step": 1405
    },
    {
      "epoch": 0.802275960170697,
      "grad_norm": 0.5812188982963562,
      "learning_rate": 3.949914627205464e-05,
      "loss": 1.7278,
      "step": 1410
    },
    {
      "epoch": 0.8051209103840683,
      "grad_norm": 0.6270999908447266,
      "learning_rate": 3.8929994308480365e-05,
      "loss": 1.7453,
      "step": 1415
    },
    {
      "epoch": 0.8079658605974396,
      "grad_norm": 0.6243561506271362,
      "learning_rate": 3.836084234490609e-05,
      "loss": 1.7076,
      "step": 1420
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 0.6285762786865234,
      "learning_rate": 3.779169038133182e-05,
      "loss": 1.6446,
      "step": 1425
    },
    {
      "epoch": 0.813655761024182,
      "grad_norm": 0.6053926348686218,
      "learning_rate": 3.722253841775754e-05,
      "loss": 1.7398,
      "step": 1430
    },
    {
      "epoch": 0.8165007112375533,
      "grad_norm": 0.664005696773529,
      "learning_rate": 3.6653386454183266e-05,
      "loss": 1.7601,
      "step": 1435
    },
    {
      "epoch": 0.8193456614509246,
      "grad_norm": 0.5988922119140625,
      "learning_rate": 3.6084234490608993e-05,
      "loss": 1.7517,
      "step": 1440
    },
    {
      "epoch": 0.8221906116642959,
      "grad_norm": 0.6144258975982666,
      "learning_rate": 3.551508252703472e-05,
      "loss": 1.6773,
      "step": 1445
    },
    {
      "epoch": 0.8250355618776671,
      "grad_norm": 0.5975134968757629,
      "learning_rate": 3.494593056346044e-05,
      "loss": 1.7839,
      "step": 1450
    },
    {
      "epoch": 0.8278805120910384,
      "grad_norm": 0.6148159503936768,
      "learning_rate": 3.4376778599886174e-05,
      "loss": 1.6746,
      "step": 1455
    },
    {
      "epoch": 0.8307254623044097,
      "grad_norm": 0.5948092937469482,
      "learning_rate": 3.3807626636311895e-05,
      "loss": 1.7639,
      "step": 1460
    },
    {
      "epoch": 0.833570412517781,
      "grad_norm": 0.6494870781898499,
      "learning_rate": 3.323847467273762e-05,
      "loss": 1.6982,
      "step": 1465
    },
    {
      "epoch": 0.8364153627311522,
      "grad_norm": 0.6689239144325256,
      "learning_rate": 3.266932270916335e-05,
      "loss": 1.6427,
      "step": 1470
    },
    {
      "epoch": 0.8392603129445235,
      "grad_norm": 0.6855570673942566,
      "learning_rate": 3.2100170745589076e-05,
      "loss": 1.7193,
      "step": 1475
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 0.5896713733673096,
      "learning_rate": 3.1531018782014796e-05,
      "loss": 1.6344,
      "step": 1480
    },
    {
      "epoch": 0.844950213371266,
      "grad_norm": 0.6127318143844604,
      "learning_rate": 3.096186681844053e-05,
      "loss": 1.75,
      "step": 1485
    },
    {
      "epoch": 0.8477951635846372,
      "grad_norm": 0.6924377679824829,
      "learning_rate": 3.039271485486625e-05,
      "loss": 1.6798,
      "step": 1490
    },
    {
      "epoch": 0.8506401137980085,
      "grad_norm": 0.6010318994522095,
      "learning_rate": 2.9823562891291977e-05,
      "loss": 1.6562,
      "step": 1495
    },
    {
      "epoch": 0.8534850640113798,
      "grad_norm": 0.6061829328536987,
      "learning_rate": 2.92544109277177e-05,
      "loss": 1.7739,
      "step": 1500
    },
    {
      "epoch": 0.8563300142247511,
      "grad_norm": 0.6011596322059631,
      "learning_rate": 2.868525896414343e-05,
      "loss": 1.7652,
      "step": 1505
    },
    {
      "epoch": 0.8591749644381224,
      "grad_norm": 0.5862594246864319,
      "learning_rate": 2.8116107000569154e-05,
      "loss": 1.6771,
      "step": 1510
    },
    {
      "epoch": 0.8620199146514936,
      "grad_norm": 0.5898637175559998,
      "learning_rate": 2.754695503699488e-05,
      "loss": 1.7037,
      "step": 1515
    },
    {
      "epoch": 0.8648648648648649,
      "grad_norm": 0.7183747887611389,
      "learning_rate": 2.6977803073420605e-05,
      "loss": 1.6927,
      "step": 1520
    },
    {
      "epoch": 0.8677098150782361,
      "grad_norm": 0.6008461713790894,
      "learning_rate": 2.6408651109846332e-05,
      "loss": 1.7761,
      "step": 1525
    },
    {
      "epoch": 0.8705547652916074,
      "grad_norm": 0.6236869096755981,
      "learning_rate": 2.5839499146272056e-05,
      "loss": 1.8027,
      "step": 1530
    },
    {
      "epoch": 0.8733997155049786,
      "grad_norm": 0.602635383605957,
      "learning_rate": 2.527034718269778e-05,
      "loss": 1.6266,
      "step": 1535
    },
    {
      "epoch": 0.8762446657183499,
      "grad_norm": 0.593680739402771,
      "learning_rate": 2.4701195219123506e-05,
      "loss": 1.6732,
      "step": 1540
    },
    {
      "epoch": 0.8790896159317212,
      "grad_norm": 0.5942068099975586,
      "learning_rate": 2.4132043255549233e-05,
      "loss": 1.7356,
      "step": 1545
    },
    {
      "epoch": 0.8819345661450925,
      "grad_norm": 0.6104512214660645,
      "learning_rate": 2.356289129197496e-05,
      "loss": 1.5765,
      "step": 1550
    },
    {
      "epoch": 0.8847795163584637,
      "grad_norm": 0.5916124582290649,
      "learning_rate": 2.2993739328400684e-05,
      "loss": 1.642,
      "step": 1555
    },
    {
      "epoch": 0.887624466571835,
      "grad_norm": 0.6343410611152649,
      "learning_rate": 2.242458736482641e-05,
      "loss": 1.6861,
      "step": 1560
    },
    {
      "epoch": 0.8904694167852063,
      "grad_norm": 0.6029444932937622,
      "learning_rate": 2.1855435401252135e-05,
      "loss": 1.6848,
      "step": 1565
    },
    {
      "epoch": 0.8933143669985776,
      "grad_norm": 0.6086158156394958,
      "learning_rate": 2.128628343767786e-05,
      "loss": 1.6082,
      "step": 1570
    },
    {
      "epoch": 0.8961593172119487,
      "grad_norm": 0.5935536026954651,
      "learning_rate": 2.071713147410359e-05,
      "loss": 1.8357,
      "step": 1575
    },
    {
      "epoch": 0.89900426742532,
      "grad_norm": 0.5620153546333313,
      "learning_rate": 2.0147979510529312e-05,
      "loss": 1.7584,
      "step": 1580
    },
    {
      "epoch": 0.9018492176386913,
      "grad_norm": 0.6914365291595459,
      "learning_rate": 1.957882754695504e-05,
      "loss": 1.9374,
      "step": 1585
    },
    {
      "epoch": 0.9046941678520626,
      "grad_norm": 0.6424348950386047,
      "learning_rate": 1.9009675583380766e-05,
      "loss": 1.8269,
      "step": 1590
    },
    {
      "epoch": 0.9075391180654339,
      "grad_norm": 0.609414279460907,
      "learning_rate": 1.8440523619806486e-05,
      "loss": 1.7911,
      "step": 1595
    },
    {
      "epoch": 0.9103840682788051,
      "grad_norm": 0.610790491104126,
      "learning_rate": 1.7871371656232213e-05,
      "loss": 1.7201,
      "step": 1600
    },
    {
      "epoch": 0.9132290184921764,
      "grad_norm": 0.5931333899497986,
      "learning_rate": 1.730221969265794e-05,
      "loss": 1.7314,
      "step": 1605
    },
    {
      "epoch": 0.9160739687055477,
      "grad_norm": 0.5790615081787109,
      "learning_rate": 1.6733067729083664e-05,
      "loss": 1.7839,
      "step": 1610
    },
    {
      "epoch": 0.918918918918919,
      "grad_norm": 0.5849339365959167,
      "learning_rate": 1.616391576550939e-05,
      "loss": 1.7412,
      "step": 1615
    },
    {
      "epoch": 0.9217638691322901,
      "grad_norm": 0.675454318523407,
      "learning_rate": 1.5594763801935118e-05,
      "loss": 1.7731,
      "step": 1620
    },
    {
      "epoch": 0.9246088193456614,
      "grad_norm": 0.6463704705238342,
      "learning_rate": 1.5025611838360842e-05,
      "loss": 1.7191,
      "step": 1625
    },
    {
      "epoch": 0.9274537695590327,
      "grad_norm": 0.6070265173912048,
      "learning_rate": 1.4456459874786569e-05,
      "loss": 1.6906,
      "step": 1630
    },
    {
      "epoch": 0.930298719772404,
      "grad_norm": 0.6068966388702393,
      "learning_rate": 1.3887307911212294e-05,
      "loss": 1.7472,
      "step": 1635
    },
    {
      "epoch": 0.9331436699857752,
      "grad_norm": 0.6121066212654114,
      "learning_rate": 1.331815594763802e-05,
      "loss": 1.7202,
      "step": 1640
    },
    {
      "epoch": 0.9359886201991465,
      "grad_norm": 0.5964875817298889,
      "learning_rate": 1.2749003984063745e-05,
      "loss": 1.7678,
      "step": 1645
    },
    {
      "epoch": 0.9388335704125178,
      "grad_norm": 0.6292905807495117,
      "learning_rate": 1.2179852020489472e-05,
      "loss": 1.812,
      "step": 1650
    },
    {
      "epoch": 0.9416785206258891,
      "grad_norm": 0.5751979351043701,
      "learning_rate": 1.1610700056915197e-05,
      "loss": 1.7609,
      "step": 1655
    },
    {
      "epoch": 0.9445234708392604,
      "grad_norm": 0.5730204582214355,
      "learning_rate": 1.1041548093340922e-05,
      "loss": 1.6924,
      "step": 1660
    },
    {
      "epoch": 0.9473684210526315,
      "grad_norm": 0.6435555815696716,
      "learning_rate": 1.047239612976665e-05,
      "loss": 1.7756,
      "step": 1665
    },
    {
      "epoch": 0.9502133712660028,
      "grad_norm": 0.5942108631134033,
      "learning_rate": 9.903244166192375e-06,
      "loss": 1.7245,
      "step": 1670
    },
    {
      "epoch": 0.9530583214793741,
      "grad_norm": 0.6403872966766357,
      "learning_rate": 9.3340922026181e-06,
      "loss": 1.6603,
      "step": 1675
    },
    {
      "epoch": 0.9559032716927454,
      "grad_norm": 0.6418140530586243,
      "learning_rate": 8.764940239043825e-06,
      "loss": 1.6638,
      "step": 1680
    },
    {
      "epoch": 0.9587482219061166,
      "grad_norm": 0.5965096354484558,
      "learning_rate": 8.195788275469552e-06,
      "loss": 1.7562,
      "step": 1685
    },
    {
      "epoch": 0.9615931721194879,
      "grad_norm": 0.5843390226364136,
      "learning_rate": 7.626636311895276e-06,
      "loss": 1.6553,
      "step": 1690
    },
    {
      "epoch": 0.9644381223328592,
      "grad_norm": 0.6538670659065247,
      "learning_rate": 7.057484348321002e-06,
      "loss": 1.6158,
      "step": 1695
    },
    {
      "epoch": 0.9672830725462305,
      "grad_norm": 0.5770142078399658,
      "learning_rate": 6.488332384746727e-06,
      "loss": 1.6901,
      "step": 1700
    }
  ],
  "logging_steps": 5,
  "max_steps": 1757,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.450120267033242e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
